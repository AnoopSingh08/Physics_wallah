{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca3ffc2-d2dd-4d6e-8324-b0a58c14dc92",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a2afd-3bf4-4fba-84cf-9bd92ce4de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithms are unsupervised machine learning techniques that aim to group similar data points together based on certain criteria, typically without prior knowledge of class labels. There are several types of clustering algorithms, each with its own approach and underlying assumptions. Here are some of the most common types of clustering algorithms:\n",
    "\n",
    "#     Centroid-based Clustering:\n",
    "#         K-Means: K-Means is perhaps the most well-known clustering algorithm. It partitions data into K clusters, where K is a user-defined parameter. It aims to minimize the within-cluster variance by iteratively updating cluster centroids.\n",
    "#         K-Means++: An improvement over K-Means, K-Means++ initializes cluster centroids in a more intelligent way to improve convergence and avoid poor local optima.\n",
    "\n",
    "#     Hierarchical Clustering:\n",
    "#         Agglomerative Hierarchical Clustering: This method starts with each data point as its own cluster and successively merges clusters based on a linkage criterion (e.g., single, complete, or average linkage). The result is a hierarchical tree-like structure called a dendrogram.\n",
    "\n",
    "#     Density-based Clustering:\n",
    "#         DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN groups data points that are close to each other and have a sufficient number of neighbors. It can discover clusters of arbitrary shapes and is robust to noise.\n",
    "#         OPTICS (Ordering Points To Identify the Clustering Structure): OPTICS is an extension of DBSCAN that creates a reachability plot to identify clusters with varying densities.\n",
    "\n",
    "#     Distribution-based Clustering:\n",
    "#         Gaussian Mixture Models (GMM): GMM models data as a mixture of several Gaussian distributions. It assumes that data points are generated from a combination of these distributions.\n",
    "#         Expectation-Maximization (EM): EM is used to estimate parameters in GMM. It iteratively updates estimates of means and covariances to maximize the likelihood of the data.\n",
    "\n",
    "#     Fuzzy Clustering:\n",
    "#         Fuzzy C-Means (FCM): FCM is an extension of K-Means that allows data points to belong to multiple clusters with varying degrees of membership. It assigns fuzzy membership values to data points.\n",
    "\n",
    "#     Spectral Clustering:\n",
    "#         Spectral Clustering: Spectral clustering uses the eigenvalues and eigenvectors of a similarity matrix to transform the data into a lower-dimensional space. It then applies K-Means or another clustering algorithm to group the transformed data.\n",
    "\n",
    "#     Self-organizing Maps (SOM):\n",
    "#         Self-organizing Maps: SOM is a type of neural network that maps data into a low-dimensional grid while preserving the topological relationships between data points.\n",
    "\n",
    "#     Biclustering:\n",
    "#         Biclustering: This technique simultaneously clusters both rows and columns of a data matrix, aiming to identify subsets of data points that exhibit similar behavior in specific contexts.\n",
    "\n",
    "# Each type of clustering algorithm has its own strengths and weaknesses, and the choice of algorithm should depend on the specific characteristics of the data and the goals of the analysis. Additionally, clustering algorithms may make different assumptions about the shapes and sizes of clusters, the distribution of data within clusters, and the number of clusters present in the data, so it's important to select an algorithm that aligns with the underlying structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6e833-f966-4639-b246-5b0d30e9c21c",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33fb51-8ad7-4fe9-a5fc-419f4de438f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering is one of the most widely used unsupervised machine learning algorithms for partitioning a dataset into distinct, non-overlapping subgroups or clusters. The goal of K-Means is to group similar data points together and discover underlying patterns or structure within the data. Here's how K-Means works:\n",
    "\n",
    "#     Initialization:\n",
    "#         The algorithm starts by selecting the number of clusters, denoted as K, that you want to find in your data. This is a user-defined parameter, and you often have to decide it based on your domain knowledge or by using techniques like the elbow method.\n",
    "#         K initial cluster centroids are randomly chosen from the data points. These centroids represent the centers of the initial clusters.\n",
    "\n",
    "#     Assignment Step:\n",
    "#         For each data point in the dataset, the algorithm calculates the distance between that point and each of the K centroids. Common distance metrics include Euclidean distance and Manhattan distance.\n",
    "#         The data point is then assigned to the cluster whose centroid is closest to it. This assignment is based on the minimum distance criterion.\n",
    "\n",
    "#     Update Step:\n",
    "#         After assigning all data points to clusters, the algorithm updates the cluster centroids. Each new centroid is calculated as the mean (average) of all data points assigned to that cluster.\n",
    "\n",
    "#     Repeat:\n",
    "#         Steps 2 and 3 are repeated iteratively until one of the stopping criteria is met. Common stopping criteria include:\n",
    "#             A maximum number of iterations is reached.\n",
    "#             The centroids no longer change significantly between iterations (convergence).\n",
    "\n",
    "#     Result:\n",
    "#         Once the algorithm converges, the final cluster assignments and cluster centroids are obtained.\n",
    "\n",
    "# K-Means aims to minimize the within-cluster variance or sum of squared distances (SSD). In other words, it seeks to make data points within the same cluster as similar as possible and data points from different clusters as dissimilar as possible.\n",
    "\n",
    "# Key characteristics of K-Means clustering:\n",
    "\n",
    "#     K-Means is sensitive to the initial placement of cluster centroids, so multiple runs with different initializations may yield different results. To mitigate this, techniques like K-Means++ are used for better initialization.\n",
    "#     K-Means assumes that clusters are spherical, equally sized, and have similar densities. It may not perform well on datasets with irregularly shaped or differently sized clusters.\n",
    "#     K-Means can handle large datasets efficiently due to its simplicity, but it may not work well if clusters have varying densities or if there is noise in the data.\n",
    "#     The algorithm can be applied to various types of data, including numerical, categorical, and mixed data, by selecting an appropriate distance metric.\n",
    "#     It is a partitional clustering algorithm, which means each data point belongs to exactly one cluster.\n",
    "\n",
    "# After running K-Means, you can analyze the results by examining the cluster assignments, cluster centroids, and within-cluster variance. It's often used for tasks like customer segmentation, image compression, and anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63daf8e-a76d-489f-89da-49b0ad83be7e",
   "metadata": {},
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37272e5a-0722-4b68-a6f7-9992a2bc0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering is a popular and widely used clustering technique, but like any algorithm, it has its own set of advantages and limitations compared to other clustering techniques. Here's a summary:\n",
    "\n",
    "# Advantages of K-Means Clustering:\n",
    "\n",
    "#     Simplicity: K-Means is straightforward to understand and implement. Its simplicity makes it a good starting point for clustering analysis.\n",
    "\n",
    "#     Efficiency: K-Means is computationally efficient, and it can handle large datasets with many data points and features.\n",
    "\n",
    "#     Scalability: It scales well with the number of data points, making it suitable for large datasets.\n",
    "\n",
    "#     Interpretability: The resulting clusters are easy to interpret, and each data point belongs to exactly one cluster.\n",
    "\n",
    "#     Easily Adaptable: K-Means can work with various types of data (numerical, categorical, mixed) by selecting appropriate distance metrics.\n",
    "\n",
    "#     Distributed Computation: It can be parallelized and distributed, allowing it to leverage the power of modern computing clusters.\n",
    "\n",
    "# Limitations of K-Means Clustering:\n",
    "\n",
    "#     Number of Clusters (K): The user must specify the number of clusters (K) in advance, which can be challenging, especially when there's no prior knowledge about the data.\n",
    "\n",
    "#     Sensitive to Initialization: The choice of initial cluster centroids can significantly affect the final clustering results. Poor initialization can lead to convergence to suboptimal solutions. Techniques like K-Means++ help improve initialization.\n",
    "\n",
    "#     Assumption of Spherical Clusters: K-Means assumes that clusters are spherical, equally sized, and have similar densities. It may not perform well on datasets with irregularly shaped or differently sized clusters.\n",
    "\n",
    "#     Sensitive to Outliers: Outliers can disproportionately influence cluster centroids, potentially leading to the creation of outlier-dominated clusters.\n",
    "\n",
    "#     Dependence on Distance Metric: The choice of distance metric can greatly impact the clustering results. It's crucial to select an appropriate metric based on the data type and domain.\n",
    "\n",
    "#     Hard Assignments: K-Means uses hard assignments, meaning each data point belongs to exactly one cluster. This doesn't capture the uncertainty or fuzziness that might be present in some datasets.\n",
    "\n",
    "#     Lack of Robustness: K-Means may not handle noise and outliers well, and it may produce unstable results when applied to slightly perturbed versions of the same data.\n",
    "\n",
    "#     Cluster Shape and Density: K-Means is not suitable for identifying clusters with complex shapes, varying densities, or hierarchical structures. Other algorithms like DBSCAN or Gaussian Mixture Models (GMM) can handle such cases better.\n",
    "\n",
    "#     Global Optimum: K-Means seeks a local minimum of the sum of squared distances (SSD) objective function, which may not guarantee finding the global optimum. Multiple runs with different initializations are typically used to mitigate this issue.\n",
    "\n",
    "# In summary, K-Means is a valuable and efficient clustering method for many scenarios but should be chosen carefully based on the specific characteristics of your data and the goals of your analysis. It may perform well when the assumptions align with your data, but alternative clustering techniques should be considered when facing more complex or less well-behaved datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c0d409-51b9-439e-8923-dbac9b758dde",
   "metadata": {},
   "source": [
    "### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2916c7-1dfd-4c49-bfaf-9b893d051e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the optimal number of clusters in K-means clustering is a crucial step in the analysis, as it directly impacts the quality of the clustering results. There are several methods to help you find the optimal number of clusters:\n",
    "\n",
    "#     Elbow Method: The Elbow Method involves running K-means clustering with a range of values for K and then plotting the within-cluster sum of squares (WCSS) as a function of K. WCSS is a measure of the variation within each cluster. The point where the WCSS starts to level off (forming an \"elbow\" in the plot) indicates an optimal number of clusters. The idea is to choose K at the point where adding more clusters doesn't significantly reduce the WCSS.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Score: The Silhouette Score measures the quality of clusters. It considers both the cohesion (how close data points are to others in the same cluster) and separation (how far apart data points in different clusters are). Higher Silhouette Scores indicate better-defined clusters. You can calculate the Silhouette Score for different values of K and choose the K with the highest score.\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    silhouette_avg = silhouette_score(data, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--')\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Gap Statistics: Gap Statistics compare the WCSS of the clustering algorithm to that of a random clustering. The idea is to find the K value where the gap between the observed and expected WCSS is the largest.\n",
    "\n",
    "# Davies-Bouldin Index: The Davies-Bouldin Index measures the average similarity between each cluster and its most similar cluster. A lower Davies-Bouldin Index suggests better clustering. You can calculate this index for different values of K and choose the K with the lowest index.\n",
    "\n",
    "    from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "    db_scores = []\n",
    "    for k in range(2, 11):\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        db_index = davies_bouldin_score(data, labels)\n",
    "        db_scores.append(db_index)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(2, 11), db_scores, marker='o', linestyle='--')\n",
    "    plt.title('Davies-Bouldin Index')\n",
    "    plt.xlabel('Number of clusters (K)')\n",
    "    plt.ylabel('Davies-Bouldin Index')\n",
    "    plt.show()\n",
    "\n",
    "    # Visual Inspection: Sometimes, it's beneficial to visualize the data and the clustering results for different values of K to see which one makes the most sense from a domain perspective.\n",
    "\n",
    "    # Domain Knowledge: In some cases, domain knowledge or specific business requirements can guide the choice of K.\n",
    "\n",
    "    # Gap Statistics and L-method: These advanced methods are useful when the standard methods like Elbow and Silhouette Score do not yield clear results. They involve comparing different clustering quality metrics for different values of K.\n",
    "\n",
    "# It's important to note that there's no one-size-fits-all method, and the choice of the optimal number of clusters may vary depending on the dataset and the goals of the analysis. Therefore, it's often recommended to use a combination of these methods and expert judgment to make a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4632a5-4df9-4e6b-b3d3-37362762f585",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef794fb0-15c1-4996-84c3-346c5a0b099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering has a wide range of applications in various real-world scenarios. Here are some common applications and how K-means has been used to solve specific problems:\n",
    "\n",
    "#     Image Compression: K-means clustering can be used to reduce the number of colors in an image, thereby compressing it. By clustering similar colors together, you can represent the image with a smaller palette while maintaining visual quality.\n",
    "\n",
    "#     Customer Segmentation: In marketing, K-means clustering is used to segment customers based on their behavior and preferences. This helps businesses tailor marketing strategies to different customer segments.\n",
    "\n",
    "#     Anomaly Detection: K-means can be used to identify anomalies or outliers in data. Data points that are significantly different from the cluster centroids can be considered anomalies. This is useful in fraud detection and quality control.\n",
    "\n",
    "#     Document Clustering: In natural language processing (NLP), K-means is used to cluster documents with similar content. For example, news articles or customer reviews can be grouped into topics or themes.\n",
    "\n",
    "#     Recommendation Systems: K-means clustering can be applied to recommend products or content to users based on their past behavior. Users with similar preferences are grouped together, and recommendations are made based on what similar users have liked.\n",
    "\n",
    "#     Genomic Data Analysis: In bioinformatics, K-means can be used to cluster genes with similar expression patterns, helping researchers understand gene functions and disease mechanisms.\n",
    "\n",
    "#     Image Segmentation: K-means clustering can be applied to segment images into regions with similar pixel intensities. This is used in medical image analysis, object detection, and computer vision tasks.\n",
    "\n",
    "#     Market Basket Analysis: In retail, K-means can help identify groups of products that are often purchased together. This information can be used for store layout optimization and targeted marketing.\n",
    "\n",
    "#     Network Security: K-means clustering can detect network intrusions by clustering normal and abnormal network traffic patterns. Any deviation from the normal clusters can signal a security threat.\n",
    "\n",
    "#     Climate Data Analysis: Climate scientists use K-means to cluster weather or climate data to identify patterns and trends, aiding in weather forecasting and climate modeling.\n",
    "\n",
    "#     Resource Allocation: K-means can help optimize resource allocation in logistics and supply chain management. It can determine the most efficient locations for warehouses or distribution centers.\n",
    "\n",
    "#     Human Activity Recognition: In wearable technology and healthcare, K-means is used to recognize human activities from sensor data, such as detecting walking, running, or sleeping patterns.\n",
    "\n",
    "# In these applications, K-means clustering provides valuable insights, helps automate decision-making, and improves the understanding of complex datasets. However, it's important to note that K-means has limitations, such as sensitivity to the initial centroids and the assumption of spherical clusters. Depending on the specific problem and data characteristics, other clustering algorithms like hierarchical clustering or DBSCAN may be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd65be-16b9-4c67-a9b7-ef2e44f874df",
   "metadata": {},
   "source": [
    "### Question6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bef3d4-ed0c-4fda-b242-4a9c1b1f5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting the output of a K-means clustering algorithm involves understanding the structure of the clusters formed and deriving meaningful insights from them. Here's how you can interpret the output and extract insights:\n",
    "\n",
    "#     Cluster Centers (Centroids): K-means assigns each data point to the nearest cluster center. The cluster centers represent the \"average\" or \"central\" data points within each cluster. By examining the coordinates of these centroids, you can gain insights into the characteristics of each cluster.\n",
    "\n",
    "#     Cluster Size: You can determine the size of each cluster by counting the number of data points assigned to it. A cluster with a larger number of data points may be more significant or representative in your dataset.\n",
    "\n",
    "#     Cluster Separation: Evaluate how distinct and well-separated the clusters are from each other. If clusters are tightly packed and well-separated, it suggests that the data points within each cluster share common characteristics, making the clustering result more meaningful.\n",
    "\n",
    "#     Within-Cluster Sum of Squares (WCSS): WCSS measures the sum of squared distances between each data point in a cluster and its centroid. Lower WCSS values indicate that data points within a cluster are closer to the centroid, which typically means more cohesive and well-defined clusters.\n",
    "\n",
    "#     Between-Cluster Sum of Squares (BCSS): BCSS measures the sum of squared distances between cluster centroids. Larger BCSS values indicate greater separation between clusters.\n",
    "\n",
    "#     Elbow Method: To determine the optimal number of clusters (K), you can use the Elbow Method. Plot the WCSS for different values of K and look for an \"elbow\" point where the rate of decrease in WCSS starts to slow down. This can help you choose a suitable number of clusters.\n",
    "\n",
    "#     Visual Inspection: Create visualizations, such as scatter plots, to visualize the data points and their assigned clusters. This can provide an intuitive understanding of the cluster structure and any potential overlap between clusters.\n",
    "\n",
    "#     Cluster Profiles: For each cluster, analyze the characteristics of the data points it contains. This may involve examining the mean or median values of features within the cluster. For example, in customer segmentation, you might look at the average spending behavior of customers in each cluster.\n",
    "\n",
    "#     Domain Knowledge: Incorporate domain knowledge to interpret the clusters. If you have prior knowledge of the data or the problem domain, it can help you make sense of the cluster assignments and their implications.\n",
    "\n",
    "#     Business Insights: Consider the practical implications of the clusters. What do the clusters represent, and how can they be used to make informed decisions or recommendations? For example, in marketing, clusters of customers with similar behavior can inform targeted marketing strategies.\n",
    "\n",
    "# By combining these approaches and considering both statistical metrics and domain-specific insights, you can effectively interpret the output of a K-means clustering algorithm and extract valuable information from the resulting clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb14862-13bc-48a3-84f6-4a7cc2baa539",
   "metadata": {},
   "source": [
    "#### Question7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538f0af-ede1-4018-8ae7-4a161e38c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing K-means clustering can pose several challenges, but many of these challenges can be addressed with appropriate techniques and precautions. Here are some common challenges and ways to address them:\n",
    "\n",
    "#     Choosing the Optimal K: One of the primary challenges is selecting the right number of clusters (K). To address this:\n",
    "#         Use the Elbow Method: Plot the within-cluster sum of squares (WCSS) for different values of K and look for an \"elbow\" point where the rate of decrease in WCSS starts to slow down.\n",
    "#         Silhouette Score: Calculate silhouette scores for different values of K and choose the K with the highest silhouette score.\n",
    "#         Domain Knowledge: Consider domain knowledge to make an informed choice of K.\n",
    "\n",
    "#     Sensitive to Initializations: K-means clustering is sensitive to initial centroid placement, which can lead to different results with different initializations.\n",
    "#         To mitigate this, perform multiple runs of K-means with different initializations and select the result with the lowest WCSS or highest silhouette score.\n",
    "\n",
    "#     Handling Outliers: Outliers can significantly impact the centroid positions and cluster assignments.\n",
    "#         Use robust clustering techniques that are less sensitive to outliers, such as DBSCAN or hierarchical clustering.\n",
    "#         Consider preprocessing techniques like outlier detection and removal before applying K-means.\n",
    "\n",
    "#     Scaling and Standardization: K-means is sensitive to the scale of features. Features with larger scales can dominate the clustering process.\n",
    "#         Standardize or normalize the features to ensure that all dimensions contribute equally to the clustering. Common methods include z-score scaling or Min-Max scaling.\n",
    "\n",
    "#     Non-Globular Clusters: K-means assumes that clusters are spherical and equally sized, which may not hold for complex data.\n",
    "#         Use clustering algorithms designed for non-globular clusters, such as DBSCAN or Gaussian Mixture Models (GMM).\n",
    "\n",
    "#     Handling Categorical Data: K-means traditionally works with continuous numerical data, so categorical data may require preprocessing.\n",
    "#         Convert categorical data to numerical form using techniques like one-hot encoding or binary encoding.\n",
    "\n",
    "#     Computational Complexity: K-means can become computationally expensive with large datasets or high dimensions.\n",
    "#         Consider using a subset of data for initial exploration.\n",
    "#         Implement parallelization or use libraries optimized for speed, like Scikit-learn's MiniBatchKMeans.\n",
    "\n",
    "#     Interpreting Results: Determining the meaning and significance of clusters can be challenging.\n",
    "#         Use domain knowledge and visualization to interpret clusters.\n",
    "#         Evaluate cluster quality using internal metrics like silhouette score or external metrics if ground truth labels are available.\n",
    "\n",
    "#     Imbalanced Cluster Sizes: K-means may produce clusters with significantly different sizes.\n",
    "#         Consider using clustering algorithms that are not constrained by cluster size, like DBSCAN or hierarchical clustering.\n",
    "\n",
    "#     Evaluating Cluster Validity: Assessing the quality of clustering results objectively can be challenging.\n",
    "#         Use internal and external validation metrics, such as silhouette score, Davies-Bouldin index, or adjusted Rand index, when ground truth labels are available.\n",
    "\n",
    "#     Handling Missing Values: K-means typically doesn't handle missing values well.\n",
    "#         Impute missing values using appropriate techniques before clustering.\n",
    "\n",
    "# Addressing these challenges requires careful consideration of the data and problem context, as well as selecting the most suitable clustering algorithm and preprocessing techniques for the specific task at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

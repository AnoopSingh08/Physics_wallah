{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95efa12-c5e5-4b78-b6db-01bc3d8390aa",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091adceb-8bd9-4243-a516-7150f494971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression and Multiple Linear Regression are both statistical methods used to model the relationship between one or more independent variables and a dependent variable. The main difference between the two lies in the number of independent variables they involve.\n",
    "\n",
    "#    Simple Linear Regression:\n",
    "#    Simple Linear Regression involves only one independent variable and one dependent variable. It aims to establish a linear relationship between the independent variable (predictor) and the dependent variable (response). The equation for simple linear regression is typically represented as:\n",
    "\n",
    "#    y=β0+β1x+ϵ\n",
    "\n",
    "# Where:\n",
    "\n",
    "#    y is the dependent variable.\n",
    "#    x is the independent variable.\n",
    "#    β0 is the intercept (y-intercept) of the line.\n",
    "#    β1 is the slope of the line.\n",
    "#    ϵ represents the error term, accounting for the variability that the model does not explain.\n",
    "\n",
    "#Example of Simple Linear Regression:\n",
    "#Suppose we want to predict a person's weight (yy) based on their height (xx). Here, height (xx) is the only independent variable. We collect data from a sample of individuals, measure their heights and weights, and use simple linear regression to find the best-fitting line that describes the relationship between height and weight.\n",
    "\n",
    "#    Multiple Linear Regression:\n",
    "#    Multiple Linear Regression involves more than one independent variable and one dependent variable. It extends the concept of simple linear regression to accommodate multiple predictors. The equation for multiple linear regression is given by:\n",
    "\n",
    "#    y=β0+β1x1+β2x2+…+βpxp+ϵ\n",
    "#where:\n",
    "\n",
    "#   y is the dependent variable.\n",
    "#   x1,x2,…,xp are the independent variables (predictors).\n",
    "#   β0 is the intercept.\n",
    "#   β1,β2,…,βp are the coefficients for each independent variable.\n",
    "#   ϵ represents the error term.\n",
    "\n",
    "#   Example of Multiple Linear Regression:\n",
    "# Let's say we want to predict a house's sale price (y) based on its area (x1), number of bedrooms (x2), and distance from the city center (x3). Here, area, number of bedrooms, and distance from the city center are the independent variables. We gather data on various houses, including these attributes, and use multiple linear regression to create a model that takes into account all three predictors to estimate the sale price.\n",
    "\n",
    "# In summary, the key difference between simple linear regression and multiple linear regression is the number of independent variables involved. Simple linear regression deals with one predictor, while multiple linear regression deals with more than one predictor. Both techniques aim to model and understand the relationships between variables and make predictions based on those relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9847bea-ba93-4e18-afd8-44ec188fbf14",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b53d7-4242-4329-b30b-34313bab16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression comes with a set of assumptions that need to hold for the model to be valid and for the results to be reliable. These assumptions help ensure that the statistical inferences drawn from the regression analysis are meaningful and accurate. Here are the key assumptions of linear regression:\n",
    "\n",
    "#    Linearity: The relationship between the independent variables and the dependent variable should be linear. This means that the change in the dependent variable should be proportional to changes in the independent variables.\n",
    "\n",
    "#    Independence: The residuals (the differences between observed and predicted values) should be independent of each other. In other words, the errors of one observation should not be related to the errors of another observation.\n",
    "\n",
    "#    Homoscedasticity: The residuals should have constant variance across all levels of the independent variables. This means that the spread of the residuals should be roughly the same for all values of the predictors.\n",
    "\n",
    "#    Normality: The residuals should follow a normal distribution. This assumption is more critical when dealing with smaller sample sizes, as larger sample sizes tend to make this assumption less important due to the Central Limit Theorem.\n",
    "\n",
    "#    No or Little Multicollinearity: The independent variables should not be highly correlated with each other. High multicollinearity can make it challenging to isolate the individual effects of each predictor on the dependent variable.\n",
    "\n",
    "# To check whether these assumptions hold in a given dataset, you can use various techniques:\n",
    "\n",
    "#    Residual Plots: Plot the residuals (observed - predicted values) against the predicted values. Check for patterns or trends. In a well-fitted linear regression model, the residuals should be randomly scattered around zero, without any discernible pattern.\n",
    "\n",
    "#    Normality Tests: Plot a histogram of the residuals and use statistical tests (e.g., Shapiro-Wilk test, Anderson-Darling test, Kolmogorov-Smirnov test) to assess the normality of the residuals.\n",
    "\n",
    "#    Homoscedasticity Tests: Create a scatter plot of residuals against predicted values. Look for a consistent spread of residuals across all predicted values. You can also perform tests like the Breusch-Pagan test or White's test for heteroscedasticity.\n",
    "\n",
    "#    Collinearity Diagnostics: Calculate correlation coefficients between independent variables to identify potential multicollinearity issues. High correlation coefficients indicate a stronger relationship between variables, which could lead to multicollinearity problems.\n",
    "\n",
    "#    Q-Q Plots: Quantile-Quantile plots can visually compare the distribution of the residuals to a theoretical normal distribution. If the points on the Q-Q plot closely follow a straight line, the assumption of normality is more likely to hold.\n",
    "\n",
    "#    Cook's Distance: Identify influential observations that might be disproportionately affecting the model's results. These observations can impact the overall fit of the model.\n",
    "\n",
    "#    VIF (Variance Inflation Factor): Compute the VIF for each predictor to assess multicollinearity. A high VIF indicates high multicollinearity between variables.\n",
    "\n",
    "#If the assumptions are violated, it might be necessary to consider transformations of variables, removing outliers, or using more advanced regression techniques that can handle violations of these assumptions. It's important to carefully assess the assumptions and their implications to ensure the validity of your regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f826b1e-2ad5-4192-ba93-1ec45d7dc98e",
   "metadata": {},
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9b59f-f765-4f3d-9dc4-c721860687dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a linear regression model, the slope and intercept have specific interpretations that help us understand the relationship between the independent variable(s) and the dependent variable. Let's delve into their meanings and provide a real-world example.\n",
    "\n",
    "# Intercept (β₀): The intercept represents the predicted value of the dependent variable when all independent variables are zero. In many cases, this interpretation might not make sense in the context of the real world, especially if zero doesn't have any practical meaning for the independent variables.\n",
    "\n",
    "# Slope (β₁): The slope represents the change in the dependent variable associated with a one-unit change in the independent variable, assuming all other variables are held constant. It quantifies the impact of the independent variable on the dependent variable.\n",
    "\n",
    "# Here's a real-world example to illustrate these concepts:\n",
    "\n",
    "# Example: House Price Prediction\n",
    "\n",
    "# Let's consider a scenario where we want to predict the price of a house based on its size (in square feet). We collect data on various houses, including their sizes and prices, and perform a simple linear regression analysis to model this relationship.\n",
    "\n",
    "# The linear regression equation would be:\n",
    "\n",
    "# Price=β0+β1×Size+ϵ\n",
    "\n",
    "#    Price is the dependent variable (house price).\n",
    "#    Size is the independent variable (house size in square feet).\n",
    "#    β0 is the intercept.\n",
    "#    β1 is the slope.\n",
    "#    ϵ represents the error term.\n",
    "\n",
    "# Interpretations:\n",
    "\n",
    "#    Intercept (β₀): In this context, the intercept (β0) is the estimated house price when the size of the house is zero. However, this interpretation is not meaningful, as a house with zero size doesn't exist. So, we usually don't focus much on the intercept's interpretation in scenarios like these.\n",
    "\n",
    "#    Slope (β₁): The slope (β1) is the change in the house price for a one-unit increase in the house size (square feet). For instance, if the slope is estimated to be $150, then a one-square-foot increase in house size is associated with an estimated increase of $150 in the house price, assuming all other factors remain constant.\n",
    "\n",
    "#If the estimated slope is negative (e.g., -150), it would mean that larger houses are associated with lower prices. If the slope is positive (e.g., 150), larger houses would be associated with higher prices.\n",
    "\n",
    "#Keep in mind that the interpretations of intercept and slope are specific to the context of the problem you are solving. In more complex scenarios involving multiple independent variables, the interpretation of the slope becomes the change in the dependent variable for a one-unit change in the independent variable, while holding all other variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c78656-61c5-4048-aab5-984c616dd3ca",
   "metadata": {},
   "source": [
    "### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07265877-7bf6-4a7b-bf53-85ac811e299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent is an optimization algorithm used to minimize (or maximize) a function by iteratively adjusting the parameters in the direction that leads to the steepest decrease (or increase) of the function. It's a fundamental technique used in various fields, including machine learning, to find the optimal values of model parameters that minimize a cost function.\n",
    "\n",
    "# In the context of machine learning, Gradient Descent is primarily used to train models such as linear regression, neural networks, support vector machines, and more. These models involve finding the best set of parameters that minimize the difference between predicted values and actual target values. This process is often formulated as an optimization problem where the goal is to find the parameters that minimize a cost function.\n",
    "\n",
    "# Here's how Gradient Descent works in machine learning:\n",
    "\n",
    "#    Initialize Parameters: Start with an initial guess for the model's parameters. These parameters determine the shape of the model and influence its predictions.\n",
    "\n",
    "#    Compute Gradient: Calculate the gradient of the cost function with respect to each parameter. The gradient represents the direction and magnitude of the steepest increase of the cost function.\n",
    "\n",
    "#    Update Parameters: Adjust the parameters by taking a step in the opposite direction of the gradient. The step size is determined by a parameter called the learning rate, which controls the size of the steps taken during each iteration.\n",
    "\n",
    "#    Repeat: Continue iterating steps 2 and 3 until the algorithm converges to a point where the cost function reaches a minimum (or maximum) value. Convergence is usually determined based on a predefined number of iterations or when the change in parameters becomes sufficiently small.\n",
    "\n",
    "# Gradient Descent comes in different variations based on how the parameters are updated at each iteration:\n",
    "\n",
    "#    Batch Gradient Descent: In each iteration, the gradient is computed using the entire dataset. It provides a globally accurate estimate of the gradient but can be computationally expensive for large datasets.\n",
    "\n",
    "#    Stochastic Gradient Descent (SGD): In each iteration, the gradient is calculated using only one randomly selected data point. It's faster than Batch Gradient Descent and is particularly useful when dealing with large datasets. However, it can exhibit more erratic behavior due to the randomness of individual data points.\n",
    "\n",
    "#    Mini-Batch Gradient Descent: A compromise between Batch Gradient Descent and SGD. It computes the gradient using a small subset (mini-batch) of the dataset. This approach balances accuracy and efficiency and is commonly used in practice.\n",
    "\n",
    "# The main idea behind Gradient Descent is that by repeatedly adjusting the model's parameters in the direction that reduces the cost function, the algorithm gradually converges to the optimal parameter values that result in the best possible model fit to the data.\n",
    "\n",
    "# While Gradient Descent is a powerful optimization technique, it's important to set an appropriate learning rate and consider techniques like learning rate schedules and momentum to enhance its convergence properties and overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04a5ff-fdc9-4e54-818c-1a2a73abac06",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f3364-65ee-4c3b-8dad-2190ec7fb221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable and multiple independent variables. In simple linear regression, we work with one independent variable to predict the dependent variable, while in multiple linear regression, we work with two or more independent variables.\n",
    "\n",
    "# The multiple linear regression model can be represented by the following equation:\n",
    "\n",
    "# y=β0+β1x1+β2x2+…+βpxp+ϵ\n",
    "\n",
    "# Where:\n",
    "\n",
    "#    y is the dependent variable.\n",
    "#    x1,x2,…,xp are the independent variables.\n",
    "#    β0 is the intercept.\n",
    "#    β1,β2,…,βp are the coefficients for each independent variable.\n",
    "#    ϵ represents the error term.\n",
    "\n",
    "# Here are the key differences between simple linear regression and multiple linear regression:\n",
    "\n",
    "#    Number of Independent Variables:\n",
    "#        In simple linear regression, there's only one independent variable (x).\n",
    "#        In multiple linear regression, there are two or more independent variables (x1,x2,…,xp).\n",
    "\n",
    "#    Equation and Interpretation:\n",
    "#        In simple linear regression, the equation is y=β0+β1x+ϵ. The interpretation of β1 is the change in yy associated with a one-unit change in x, while keeping other factors constant.\n",
    "#        In multiple linear regression, the equation includes multiple terms for each independent variable (x1,x2,…,xp), and the interpretation of each βi coefficient is the change in y associated with a one-unit change in xi, while keeping other variables constant.\n",
    "\n",
    "#    Complexity and Flexibility:\n",
    "#        Simple linear regression models are simpler to interpret and understand, making them suitable for scenarios where the relationship between the dependent and independent variable is relatively straightforward.\n",
    "#        Multiple linear regression models can capture more complex relationships by incorporating multiple predictors. They are suitable for cases where the outcome depends on multiple factors.\n",
    "\n",
    "#    Model Performance and Overfitting:\n",
    "#        Multiple linear regression can potentially result in overfitting if too many independent variables are included without proper justification or regularization techniques.\n",
    "#        Simple linear regression may be less prone to overfitting due to the presence of fewer variables.\n",
    "\n",
    "#    Data Requirements:\n",
    "#        In both cases, assumptions like linearity, independence of residuals, and normality of errors hold. However, multiple linear regression is more sensitive to multicollinearity (high correlation between independent variables), which can affect coefficient estimates and model interpretability.\n",
    "\n",
    "# Overall, multiple linear regression is a versatile tool that allows for the incorporation of multiple predictors in the model, providing a more comprehensive understanding of the relationship between the dependent and independent variables. However, its complexity requires careful consideration of variable selection, multicollinearity, and model evaluation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1ac38-6b83-48ba-aab1-47985222acb7",
   "metadata": {},
   "source": [
    "### Question6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f31d0-5922-485c-a54b-62d923d66cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity is a phenomenon that occurs when two or more independent variables in a multiple linear regression model are highly correlated with each other. This high correlation can lead to issues in the model estimation and interpretation, making it difficult to isolate the individual effects of each variable on the dependent variable. In other words, multicollinearity makes it challenging to distinguish the separate influences of correlated variables on the outcome.\n",
    "\n",
    "# Multicollinearity can manifest in several ways:\n",
    "\n",
    "#    High Correlation Coefficients: The correlation coefficients between pairs of independent variables are close to 1 or -1. This indicates a strong linear relationship between variables.\n",
    "\n",
    "#    Large Variance Inflation Factors (VIF): VIF measures how much the variance of the estimated regression coefficient is inflated due to multicollinearity. A high VIF value (typically above 5 or 10) suggests that multicollinearity might be problematic.\n",
    "\n",
    "#    Unstable Coefficient Estimates: Small changes in the dataset can lead to large changes in the estimated coefficients.\n",
    "\n",
    "#    Conflicting Signs of Coefficients: When two correlated independent variables have opposite signs in their coefficients, it becomes challenging to interpret their effects.\n",
    "\n",
    "#Effects of Multicollinearity:\n",
    "\n",
    "#    Unreliable Coefficient Estimates: Multicollinearity can inflate the standard errors of the coefficient estimates, leading to imprecise and statistically insignificant estimates.\n",
    "\n",
    "#    Misleading Interpretations: It becomes difficult to interpret the individual impact of correlated variables on the dependent variable, as their effects cannot be separated.\n",
    "\n",
    "#Detection and Addressing Multicollinearity:\n",
    "\n",
    "#    Correlation Matrix: Calculate the correlation matrix of the independent variables. High absolute values (close to 1 or -1) suggest potential multicollinearity.\n",
    "\n",
    "#    VIF Calculation: Calculate the VIF for each independent variable. High VIF values indicate the presence of multicollinearity.\n",
    "\n",
    "#To address multicollinearity:\n",
    "\n",
    "#    Feature Selection: Remove one of the highly correlated variables. Choose the variable that makes more sense conceptually or empirically.\n",
    "\n",
    "#    Feature Transformation: Combine or create new variables that capture the essence of the correlated variables, thus reducing multicollinearity.\n",
    "\n",
    "#    Regularization Techniques: Techniques like Ridge Regression and Lasso Regression add penalty terms to the cost function, discouraging large coefficient estimates and mitigating multicollinearity.\n",
    "\n",
    "#    Collect More Data: A larger dataset can help mitigate multicollinearity by providing more variation and reducing the likelihood of high correlations.\n",
    "\n",
    "#    Domain Knowledge: Use domain expertise to understand whether correlated variables are truly redundant or if they provide unique information.\n",
    "\n",
    "#    Principal Component Analysis (PCA): PCA transforms the original variables into a new set of orthogonal variables that are linear combinations of the original ones, helping reduce multicollinearity.\n",
    "\n",
    "# It's important to note that not all levels of multicollinearity will significantly affect the model's performance. However, severe multicollinearity can lead to unreliable model results and interpretations. Therefore, detecting and addressing multicollinearity is crucial for building accurate and interpretable multiple linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fa45b-65dd-4bac-8fb5-cac658a8936b",
   "metadata": {},
   "source": [
    "### Question7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06344ebf-c97e-4266-83a4-77e86cf4747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression is a form of regression analysis that allows for modeling relationships between variables using polynomial functions. Unlike linear regression, where the relationship between the dependent and independent variables is assumed to be linear, polynomial regression can capture nonlinear relationships by introducing higher-degree polynomial terms.\n",
    "\n",
    "# In a polynomial regression model, the relationship between the dependent variable (yy) and the independent variable (xx) is represented as:\n",
    "\n",
    "# y=β0+β1x+β2x2+β3x3+…+βnxn+ϵ\n",
    "# Where:\n",
    "\n",
    "#    y is the dependent variable.\n",
    "#    x is the independent variable.\n",
    "#    β0,β1,…,βn are the coefficients for each term.\n",
    "#    n is the degree of the polynomial, determining the highest power of x in the equation.\n",
    "#    ϵ represents the error term.\n",
    "\n",
    "# Key differences between Polynomial Regression and Linear Regression:\n",
    "\n",
    "#    Functional Form:\n",
    "#        In Linear Regression, the relationship between variables is assumed to be linear (y=β0+β1x+ϵy=β0​+β1​x+ϵ).\n",
    "#        In Polynomial Regression, the relationship is modeled as a polynomial equation, allowing for curves and nonlinear patterns.\n",
    "\n",
    "#    Degree of Flexibility:\n",
    "#        Linear Regression is simpler and assumes a constant rate of change between variables.\n",
    "#        Polynomial Regression is more flexible and can capture varying rates of change and nonlinear patterns.\n",
    "\n",
    "#    Model Complexity:\n",
    "#        Linear Regression models are simpler and have fewer parameters.\n",
    "#        Polynomial Regression models become more complex as the degree of the polynomial increases, requiring more coefficients to estimate.\n",
    "\n",
    "#    Overfitting Risk:\n",
    "#        Polynomial Regression can be prone to overfitting, especially when using high-degree polynomials. Overfitting occurs when the model fits the training data too closely and does not generalize well to new data.\n",
    "\n",
    "#    Model Interpretability:\n",
    "#        Linear Regression models are generally easier to interpret due to their simple and direct relationship between variables.\n",
    "#        Polynomial Regression models can be more difficult to interpret, especially with higher-degree polynomials, as the relationships become more complex.\n",
    "\n",
    "# When to Use Polynomial Regression:\n",
    "\n",
    "#Polynomial Regression is suitable when there is a clear indication that the relationship between the variables is nonlinear. It allows you to capture curves, bends, and fluctuations that cannot be captured by linear models. However, it's important to carefully select the degree of the polynomial to avoid overfitting. Techniques like cross-validation can help determine an appropriate degree that balances model complexity and generalization.\n",
    "\n",
    "# In summary, Polynomial Regression extends the concept of Linear Regression by introducing polynomial terms to capture nonlinear relationships between variables. It offers increased flexibility but requires careful consideration of model complexity and the potential for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffdf46-7b99-4c03-b826-061abe2f126a",
   "metadata": {},
   "source": [
    "### Question8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a72cd8-0d97-4958-a5cf-d9e5f775ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "#    Flexibility: Polynomial Regression can capture more complex and nonlinear relationships between variables, allowing it to fit data that doesn't follow a linear pattern.\n",
    "\n",
    "#    Better Fit to Data: When the data exhibits curves, bends, or fluctuations, Polynomial Regression can provide a better fit compared to a linear model.\n",
    "\n",
    "#    Higher Order Patterns: Polynomial Regression can capture higher-order patterns that might be missed by linear models, enabling the model to capture intricate relationships.\n",
    "\n",
    "# Disadvantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "#    Overfitting Risk: High-degree polynomials can lead to overfitting, where the model fits the training data too closely and fails to generalize well to new data.\n",
    "\n",
    "#    Complexity: As the degree of the polynomial increases, the model becomes more complex with more parameters to estimate. This can lead to difficulties in interpretation and increased computational requirements.\n",
    "\n",
    "#    Less Generalization: While Polynomial Regression can fit training data well, it might struggle to generalize to new data, especially when the model is too complex.\n",
    "\n",
    "#    Interpretability: Higher-degree polynomials can result in intricate relationships that are difficult to interpret, unlike the simplicity of linear relationships in Linear Regression.\n",
    "\n",
    "# When to Prefer Polynomial Regression:\n",
    "\n",
    "#    Nonlinear Relationships: When there's evidence or domain knowledge suggesting that the relationship between variables is nonlinear, Polynomial Regression is a better choice.\n",
    "\n",
    "#    Curves and Bends: If the data shows curves, bends, or oscillations that a linear model cannot capture, Polynomial Regression can provide a more accurate representation.\n",
    "\n",
    "#    Small Sample Size: In cases where the sample size is small and the relationship appears to be nonlinear, using Polynomial Regression might provide a better fit than trying to force a linear model.\n",
    "\n",
    "#    Exploratory Analysis: Polynomial Regression can be useful in exploratory data analysis when you're uncertain about the exact nature of the relationship and want to visualize potential nonlinear patterns.\n",
    "\n",
    "#    Regularization: Techniques like Ridge Regression or Lasso Regression can be used with Polynomial Regression to control overfitting by adding penalty terms to the high-degree polynomial coefficients.\n",
    "\n",
    "# In summary, Polynomial Regression offers increased flexibility and can capture complex relationships that Linear Regression cannot. However, it comes with the trade-offs of increased complexity and the risk of overfitting. Careful consideration of the data's characteristics, model complexity, and regularization techniques is necessary when deciding whether to use Polynomial Regression or Linear Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

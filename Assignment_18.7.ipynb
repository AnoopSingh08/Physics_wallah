{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5d2039-5b68-44dd-be72-a094cf273435",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa506b-8106-4839-986e-3461b21639b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform the tasks you've mentioned, you can follow these steps using Python and the scikit-learn library:\n",
    "\n",
    "#    Download the Wine Dataset:\n",
    "#    You can download the Wine dataset from the UCI Machine Learning Repository using the following code:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the dataset\n",
    "wine_data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "\n",
    "# Define column names for the dataset\n",
    "column_names = [\n",
    "    \"Class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic Acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of Ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total Phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid Phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color Intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of Diluted Wines\",\n",
    "    \"Proline\",\n",
    "]\n",
    "\n",
    "# Read the dataset into a Pandas dataframe\n",
    "wine_df = pd.read_csv(wine_data_url, header=None, names=column_names)\n",
    "\n",
    "# Split the Dataset:\n",
    "# Split the dataset into features (X) and the target variable (y). In this dataset, the \"Class\" column represents the target variable, and the rest are features.\n",
    "\n",
    "X = wine_df.drop(\"Class\", axis=1)\n",
    "y = wine_df[\"Class\"]\n",
    "\n",
    "# Data Preprocessing:\n",
    "# Depending on the dataset's characteristics, you may need to perform preprocessing steps like scaling and missing value imputation. Since this dataset is commonly used, it usually comes preprocessed, but you should check for any missing values and scaling requirements.\n",
    "\n",
    "# Implement PCA:\n",
    "# You can implement PCA using scikit-learn as follows:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance with the desired number of components\n",
    "pca = PCA(n_components=2)  # You can change the number of components\n",
    "\n",
    "# Fit and transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Determine Optimal Number of Components:\n",
    "# To determine the optimal number of components to retain based on the explained variance ratio, you can use the following code:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit PCA with all components\n",
    "pca_all = PCA()\n",
    "pca_all.fit(X)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "explained_variance_ratio = pca_all.explained_variance_ratio_\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio.cumsum(), marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"Explained Variance Ratio vs. Number of Components\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# In the resulting plot, you can visually inspect the point where the explained variance starts to level off. This can help you decide on the number of components to retain.\n",
    "\n",
    "# Visualize the Results:\n",
    "# After choosing the number of components to retain, you can visualize the PCA results using a scatter plot:\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"viridis\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA Visualization\")\n",
    "plt.colorbar(label=\"Wine Class\")\n",
    "plt.show()\n",
    "\n",
    "# Perform Clustering with K-Means:\n",
    "# You can perform clustering on the PCA-transformed data using the K-Means clustering algorithm:\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a K-Means instance\n",
    "kmeans = KMeans(n_clusters=3)  # Adjust the number of clusters as needed\n",
    "\n",
    "# Fit K-Means to the PCA-transformed data\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Assign cluster labels to data points\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Interpret Results:\n",
    "# The PCA visualization and clustering results can help you understand the structure and relationships within the dataset. The scatter plot allows you to visualize how data points cluster in the reduced-dimensional space, while the clustering results (cluster_labels) provide information about which data points belong to each cluster.\n",
    "\n",
    "# You can further evaluate the clustering results using metrics like silhouette score or by comparing the clusters to the original class labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

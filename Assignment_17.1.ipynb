{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92f8e51-5b39-4e99-aaf8-682577536bcc",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12003e7-0bf7-42db-8201-eed39fb5a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' theorem is a fundamental concept in probability theory and statistics that describes the relationship between conditional probabilities of events. It provides a way to update the probability of an event based on new information or evidence. The theorem is named after the Reverend Thomas Bayes, an 18th-century mathematician and theologian who developed the concept.\n",
    "\n",
    "# Mathematically, Bayes' theorem can be expressed as follows:\n",
    "\n",
    "# P(A∣B)=P(B∣A) * P(A)/P(B)\n",
    "\n",
    "# Where:\n",
    "\n",
    "#    P(A∣B) is the conditional probability of event A occurring given that event B has occurred.\n",
    "#    P(B∣A) is the conditional probability of event B occurring given that event A has occurred.\n",
    "#    P(A) is the prior probability of event A occurring.\n",
    "#    P(B) is the prior probability of event B occurring.\n",
    "\n",
    "# Bayes' theorem is used in various fields, including statistics, machine learning, and artificial intelligence, to update beliefs or make predictions based on new evidence or observations. It plays a central role in Bayesian inference, which involves using prior knowledge and data to make probabilistic inferences about unknown quantities or events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa0bb8-939c-4576-8bce-497fcdde9b5f",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef276e1-b843-4c35-b576-d4a4c0b2fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' theorem is expressed as:\n",
    "\n",
    "# P(A∣B)=P(B∣A)⋅P(A)/P(B)\n",
    "\n",
    "# Where:\n",
    "\n",
    "#    P(A∣B) is the conditional probability of event A occurring given that event B has occurred.\n",
    "#    P(B∣A) is the conditional probability of event B occurring given that event A has occurred.\n",
    "#    P(A) is the prior probability of event A occurring.\n",
    "#    P(B) is the prior probability of event B occurring.\n",
    "\n",
    "# This formula provides a way to update the probability of an event based on new evidence or observations. It is widely used in statistics, probability theory, and various fields to make probabilistic inferences and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59c3ee-ac6f-4c03-932a-59f501e1caef",
   "metadata": {},
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9d519-d758-4280-bb9c-f641de97c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' theorem is used in a wide range of practical applications, especially in fields involving uncertainty, probability, and decision-making. Here are some common ways Bayes' theorem is used in practice:\n",
    "\n",
    "#    Medical Diagnostics: In medical diagnosis, Bayes' theorem is used to calculate the probability of a person having a certain disease given the results of medical tests. It helps doctors determine the likelihood of a disease based on observed symptoms and test outcomes.\n",
    "\n",
    "#    Spam Filtering: Email spam filters use Bayes' theorem to classify incoming emails as spam or not spam. The theorem helps update the probability of an email being spam based on certain keywords and patterns found in the email content.\n",
    "\n",
    "#    Machine Learning: Bayes' theorem is used in various machine learning algorithms, such as Naive Bayes classifiers. These classifiers predict the probability of a particular class label given the observed features, and they're used in text classification, sentiment analysis, and more.\n",
    "\n",
    "#    Stock Market Predictions: Bayesian models are used to make predictions in financial markets, taking into account historical data, market trends, and other relevant factors.\n",
    "\n",
    "#    Natural Language Processing: Bayes' theorem is used in language modeling tasks, such as speech recognition and machine translation, to estimate the most likely sequence of words or phrases given the input.\n",
    "\n",
    "#    A/B Testing: Bayes' theorem can be used to analyze the results of A/B tests. It helps determine if a change in a website or application feature led to a significant improvement or not.\n",
    "\n",
    "#    Forensics: Bayes' theorem is applied in forensic science to evaluate evidence and calculate the probability of certain events or scenarios based on available information.\n",
    "\n",
    "#    Weather Forecasting: Bayesian methods can be used to update weather forecasts as new data becomes available.\n",
    "\n",
    "#    Document Classification: Bayes' theorem is used in document categorization and topic modeling, where it helps classify documents into predefined categories.\n",
    "\n",
    "#    Risk Assessment: In insurance and risk assessment, Bayes' theorem helps adjust probabilities based on additional information to estimate risks accurately.\n",
    "\n",
    "# In essence, Bayes' theorem provides a way to update our beliefs and make better decisions based on new information, combining prior knowledge with observed evidence. It's a foundational concept in Bayesian statistics and probability theory that has widespread applications across different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189efdf5-b212-4fd2-9aa1-3f2af77bf633",
   "metadata": {},
   "source": [
    "### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5220c-b23d-44f5-92db-d4671bb61721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' theorem and conditional probability are closely related concepts in probability theory, and Bayes' theorem can be derived from the principles of conditional probability.\n",
    "\n",
    "# Conditional probability refers to the probability of an event occurring given that another event has already occurred. Mathematically, if A and B are two events, the conditional probability of event A occurring given that event B has occurred is denoted as P(A|B).\n",
    "\n",
    "# Bayes' theorem provides a way to update the conditional probability of an event based on new evidence. It establishes a relationship between two conditional probabilities. In the context of two events A and B, Bayes' theorem can be stated as:\n",
    "\n",
    "# P(A∣B)=P(B∣A)*P(A)/P(B)\n",
    "\n",
    "# Where:\n",
    "\n",
    "#    P(A∣B) is the updated probability of event A occurring given that event B has occurred.\n",
    "#    P(B∣A) is the probability of event B occurring given that event A has occurred.\n",
    "#    P(A) is the prior probability of event A occurring.\n",
    "#    P(B) is the prior probability of event B occurring.\n",
    "\n",
    "# In other words, Bayes' theorem allows us to calculate the probability of one event given the occurrence of another event, by incorporating prior knowledge (prior probability) and new evidence (conditional probability). It provides a formal framework for updating our beliefs in light of new information.\n",
    "\n",
    "# Conditional probability is a fundamental concept used to derive Bayes' theorem, and Bayes' theorem, in turn, provides a method to update and refine conditional probabilities based on observed data. This relationship is widely used in various fields for making decisions, predictions, and inferences under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b86184-5745-461c-8b47-0eb93032c09a",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f02a28-b8a0-4ed1-9021-6957c3c5c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the appropriate type of Naive Bayes classifier for a given problem depends on the nature of the data and the assumptions that can be made about the features. There are three main types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's how you can decide which one to use:\n",
    "\n",
    "#    Gaussian Naive Bayes:\n",
    "#        Use when your data features follow a Gaussian (normal) distribution.\n",
    "#        Suitable for continuous or numerical features.\n",
    "#        Assumes that each class is associated with a Gaussian distribution for each feature.\n",
    "#        Commonly used for problems involving continuous features, such as in natural language processing (NLP) applications where features like word frequencies are typically continuous.\n",
    "\n",
    "#    Multinomial Naive Bayes:\n",
    "#        Use when your data involves discrete features that represent counts or frequencies, like word occurrences in a document.\n",
    "#        Suitable for text classification and NLP tasks.\n",
    "#        Assumes a multinomial distribution for each feature in each class.\n",
    "#        Often used for document classification, spam detection, sentiment analysis, and other text-related tasks.\n",
    "\n",
    "#    Bernoulli Naive Bayes:\n",
    "#        Use when your data involves binary features, where each feature is either present (1) or absent (0).\n",
    "#        Suitable for problems involving binary or Boolean data.\n",
    "#        Assumes a Bernoulli distribution for each feature in each class.\n",
    "#        Useful for tasks like text classification where you're interested in whether a certain word occurs in a document or not.\n",
    "\n",
    "# In addition to the distribution of features, consider the size of your dataset and the nature of the problem. Naive Bayes classifiers are relatively simple and efficient algorithms, making them suitable for large datasets. However, they do make the assumption of feature independence (hence the term \"naive\"), which might not hold true for all problems. Despite this assumption, Naive Bayes classifiers can perform surprisingly well in practice, especially when the data is well-suited to the chosen type of classifier.\n",
    "\n",
    "# It's often a good idea to experiment with different types of Naive Bayes classifiers on your dataset and evaluate their performance using appropriate metrics. Cross-validation can help you determine which type works best for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78ee3c-5493-4b61-be62-9b1bd82c231b",
   "metadata": {},
   "source": [
    "### Question6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013eeda-749d-4e6f-a49f-18a72c3116af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict the class of the new instance (X1=3, X2=4) using Naive Bayes, we need to calculate the conditional probabilities for each class based on the given feature frequencies. We'll calculate the probabilities for both classes (A and B) and then choose the class with the higher probability.\n",
    "\n",
    "# Let's denote the event that the new instance belongs to class A as \"A\" and the event that it belongs to class B as \"B.\"\n",
    "\n",
    "# We can use the Naive Bayes formula:\n",
    "\n",
    "# P(A∣X1=3,X2=4)=P(X1=3∣A)×P(X2=4∣A)×P(A)/P(X1=3,X2=4)\n",
    "\n",
    "# P(B∣X1=3,X2=4)=P(X1=3∣B)×P(X2=4∣B)×P(B)/P(X1=3,X2=4)\n",
    "\n",
    "#Since the prior probabilities of classes A and B are equal (equal prior probabilities), we can ignore the P(A) and P(B) terms in the numerator.\n",
    "\n",
    "# Let's calculate the individual probabilities:\n",
    "\n",
    "#    P(X1=3∣A)=4/13\n",
    "\n",
    "#    P(X2=4∣A)=3/13\n",
    "\n",
    "#    P(X1=3∣B)=1/7\n",
    "\n",
    "#    P(X2=4∣B)=3/7\n",
    "\n",
    "#Now, let's calculate the denominator P(X1=3,X2=4):\n",
    "\n",
    "#    P(X1=3,X2=4)=P(X1=3,X2=4∣A)×P(A)+P(X1=3,X2=4∣B)×P(B)\n",
    "\n",
    "#    P(X1=3,X2=4∣A)=4/3×3/3\n",
    "\n",
    "#    P(X1=3,X2=4∣B)=1/7*3/7\n",
    "\n",
    "#Finally, substitute the values into the Naive Bayes formula:\n",
    "\n",
    "#    P(A∣X1=3,X2=4)= 4/13 * 3/13 /(P(x1=3,x2=4))\n",
    "#    P(B∣X1=3,X2=4)= 1/7 * 3/7 /(P(x1=3,x2=4))\n",
    "\n",
    "# Calculate P(A∣X1=3,X2=4) and P(B∣X1=3,X2=4) and choose the class with the higher probability. The class with the higher probability would be the predicted class for the new instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

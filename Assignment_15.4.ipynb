{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88ef6f5-4be2-472d-acca-d6cd1cc132f3",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6fd6a-377e-4970-ba21-260a1ad27572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression, which stands for \"Least Absolute Shrinkage and Selection Operator,\" is a linear regression technique that adds an L1 regularization penalty to the ordinary least squares (OLS) cost function. This penalty encourages the model to have sparse coefficients by driving some coefficients to become exactly zero. In essence, Lasso Regression not only aims to fit the data but also performs feature selection by excluding some variables from the model.\n",
    "\n",
    "# Key characteristics and differences of Lasso Regression compared to other regression techniques include:\n",
    "\n",
    "#    Regularization Penalty:\n",
    "#    Lasso Regression adds an L1 regularization penalty to the cost function, which is the absolute sum of the coefficients: λ∑i=1 to p ∣βi∣. This penalty encourages smaller coefficient values and forces some coefficients to be exactly zero, effectively selecting a subset of features.\n",
    "\n",
    "#    Feature Selection:\n",
    "#    Unlike Ridge Regression, which shrinks all coefficients towards zero but does not force them to be zero, Lasso has an inherent feature selection property. It can exclude less important features by setting their coefficients to zero.\n",
    "\n",
    "#    Sparsity and Simplicity:\n",
    "#    Lasso tends to yield sparse models with fewer active predictor variables. This can improve model interpretability and potentially lead to simpler and more easily understandable models.\n",
    "\n",
    "#    Coefficient Shrinkage:\n",
    "#    Lasso's regularization effect shrinks coefficients towards zero, which helps mitigate overfitting by reducing the complexity of the model.\n",
    "\n",
    "#    Multicollinearity Handling:\n",
    "#    Lasso is effective in handling multicollinearity as it tends to select one variable from a group of highly correlated variables and set the others to zero. This can be advantageous in situations with high collinearity.\n",
    "\n",
    "#    Regularization Parameter Selection:\n",
    "#    Similar to Ridge Regression, Lasso requires the selection of a regularization parameter (λ). The optimal λ controls the strength of the regularization and impacts the extent of coefficient shrinkage and feature selection.\n",
    "\n",
    "#    Comparison with Ridge Regression:\n",
    "#    Lasso and Ridge Regression are similar in that they both introduce regularization, but they differ in the type of penalty. Ridge's L2 penalty encourages smaller coefficients but does not lead to exact zero coefficients. Lasso's L1 penalty leads to sparse models by driving some coefficients to zero.\n",
    "\n",
    "#    Other Regression Techniques:\n",
    "#    Lasso is one of several regularization techniques, including Ridge Regression, Elastic Net (a combination of Ridge and Lasso penalties), and more advanced methods like Support Vector Regression with L1 penalty.\n",
    "\n",
    "#    Interpretation:\n",
    "#    Interpretation of Lasso coefficients can be challenging due to their sparse nature. Non-zero coefficients indicate influential variables, while zero coefficients suggest excluded variables. The magnitude of non-zero coefficients still indicates the magnitude of the variable's impact on the response.\n",
    "\n",
    "#    Scaling Sensitivity:\n",
    "#    Lasso, like Ridge Regression, is sensitive to the scale of predictor variables. Proper scaling is essential to ensure a fair penalty across all variables.\n",
    "\n",
    "# In summary, Lasso Regression is a powerful regression technique that combines both fitting the data and performing feature selection through the use of an L1 regularization penalty. Its ability to yield sparse models and handle multicollinearity makes it a valuable tool for variable selection and building interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae024c-a18d-426d-b23e-c2ec322482f6",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e2903-650a-45c6-aecf-b49ae6a14fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main advantage of using Lasso Regression for feature selection is its inherent ability to perform automatic and effective feature selection by driving some coefficients to become exactly zero. This property makes Lasso Regression a powerful technique for identifying and excluding irrelevant or less important features from a predictive model. Here's why Lasso Regression is advantageous for feature selection:\n",
    "\n",
    "#    Sparse Models: Lasso Regression tends to yield sparse models, meaning that it selects a subset of predictor variables while excluding the rest. This can lead to simpler, more interpretable, and more computationally efficient models.\n",
    "\n",
    "#    Automatic Selection: Lasso automatically identifies which variables are important for predicting the response and which variables can be safely excluded. This eliminates the need for manual feature selection based on domain knowledge, reducing the risk of human bias in the selection process.\n",
    "\n",
    "#    Multicollinearity Handling: Lasso effectively handles multicollinearity by selecting one variable from a group of correlated variables and setting the others to zero. This helps prevent overfitting due to correlated features and simplifies the model.\n",
    "\n",
    "#    Reduced Model Complexity: By excluding irrelevant features, Lasso reduces the complexity of the model. This can lead to improved generalization to new data and reduced risk of overfitting.\n",
    "\n",
    "#    Enhanced Interpretability: Sparse models generated by Lasso Regression are easier to interpret and explain. With fewer active variables, the relationships between variables and the response become clearer.\n",
    "\n",
    "#    Efficient Variable Selection: In situations where the number of features is much larger than the number of observations (high-dimensional data), Lasso can efficiently select a subset of the most relevant variables.\n",
    "\n",
    "#    Avoiding Overfitting: The feature selection property of Lasso helps mitigate the risk of overfitting, which occurs when a model learns noise in the data rather than true patterns.\n",
    "\n",
    "#    Practicality in Large-Scale Analysis: In scenarios involving high-dimensional data, where the number of features exceeds the number of observations, manual feature selection becomes challenging. Lasso automates the process efficiently.\n",
    "\n",
    "#    Data-Driven Insights: Lasso can provide insights into which features have a substantial impact on the response variable, allowing you to focus on the most influential aspects of the data.\n",
    "\n",
    "#    Combining Predictive Power and Simplicity: Lasso enables the creation of models that strike a balance between predictive power and model simplicity, improving the model's ability to generalize.\n",
    "\n",
    "# Despite these advantages, it's important to note that Lasso Regression might not be suitable for all scenarios. In cases where multiple correlated features are equally important and should not be excluded simultaneously, Ridge Regression or other techniques might be more appropriate. The choice between Lasso and other methods depends on the data characteristics and the specific goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151f860-1a6e-4a6c-ad33-f6d7b4398415",
   "metadata": {},
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbdd1b-1aaf-43e7-bb56-c4a3c9ed91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting the coefficients of a Lasso Regression model can be more straightforward compared to some other regression techniques, due to Lasso's inherent feature selection property. However, there are still some nuances to consider. Here's how you can interpret the coefficients of a Lasso Regression model:\n",
    "\n",
    "#    Non-Zero Coefficients:\n",
    "#    Non-zero coefficients indicate that the corresponding predictor variables have a significant impact on the response variable. These variables are considered relevant for predicting the outcome.\n",
    "\n",
    "#    Zero Coefficients:\n",
    "#    Coefficients that are exactly zero indicate that the corresponding predictor variables have been excluded from the model. Lasso's feature selection property automatically determines that these variables do not contribute significantly to the prediction.\n",
    "\n",
    "#    Magnitude and Sign:\n",
    "#    The magnitude and sign of non-zero coefficients are interpreted similarly to those in ordinary least squares (OLS) regression. A positive coefficient suggests a positive relationship with the response variable, while a negative coefficient suggests a negative relationship.\n",
    "\n",
    "#    Comparing Magnitudes:\n",
    "#    Within the same Lasso model, you can compare the magnitudes of non-zero coefficients to gauge the relative importance of features. Larger coefficients have a stronger influence on the response compared to smaller coefficients.\n",
    "\n",
    "#    Relative Importance:\n",
    "#    Lasso's sparse nature allows you to identify which features are most influential, as only a subset of features has non-zero coefficients. Focus on these features when making predictions or drawing insights.\n",
    "\n",
    "#    Feature Exclusion:\n",
    "#    Zero coefficients indicate that the corresponding variables were excluded from the model. Interpret these exclusions as an automated selection process based on their predictive importance.\n",
    "\n",
    "#    Intercept Interpretation:\n",
    "#    Interpret the intercept (constant term) as the average predicted response when all predictor variables are zero. The magnitude of the intercept might also be influenced by the regularization effect.\n",
    "\n",
    "#    Scaling Impact:\n",
    "#    Lasso's coefficients can be influenced by the scale of predictor variables. It's important to ensure that all variables are properly scaled before fitting the model.\n",
    "\n",
    "#    Multicollinearity Handling:\n",
    "#    Lasso's feature selection property helps address multicollinearity by automatically selecting one variable from a correlated group and setting the others to zero.\n",
    "\n",
    "#    Complex Models:\n",
    "#    Keep in mind that interpreting complex models with many non-zero coefficients can become challenging. Prioritize understanding the most important variables and their impact.\n",
    "\n",
    "# In summary, interpreting the coefficients of a Lasso Regression model involves assessing both the magnitude and sign of coefficients, understanding the feature selection property, and comparing the relative importance of features within the model. Lasso's ability to automatically select and exclude variables can simplify the process of understanding the model's relationships between predictors and the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037958d0-3da3-49d1-8389-138f3298ba1c",
   "metadata": {},
   "source": [
    "### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcf773-69c3-4353-9f80-2b0e2c787de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Lasso Regression, there is typically one primary tuning parameter that can be adjusted to control the strength of regularization: the regularization parameter (λλ). The regularization parameter affects how much the model penalizes the size of the coefficients. By adjusting λλ, you can control the trade-off between fitting the data closely (low bias) and preventing overfitting (low variance). Here's how the regularization parameter affects the model's performance:\n",
    "\n",
    "#    Regularization Parameter (λ):\n",
    "#    λ is the main tuning parameter in Lasso Regression. It controls the strength of the L1 regularization penalty, which encourages some coefficients to become exactly zero. Higher values of λλ lead to stronger regularization and sparser models with more coefficients forced to zero.\n",
    "\n",
    "#        Small λ: When λ is small, the L1 regularization effect is weak, and the model's behavior approaches that of ordinary least squares (OLS) regression. The model will try to fit the data closely, potentially leading to overfitting if there are many features and relatively few observations.\n",
    "\n",
    "#        Large λ: As λ increases, the L1 regularization becomes more pronounced. The model will prioritize simplicity and regularization over fitting the data closely. This can lead to a sparse model with many coefficients set to zero, effectively performing feature selection.\n",
    "\n",
    "#        Selecting λ: The optimal λ value should be selected using techniques like cross-validation or grid search. Cross-validation helps find the λ value that balances model complexity and predictive performance on unseen data.\n",
    "\n",
    "# The choice of λ has a significant impact on the trade-off between bias and variance:\n",
    "\n",
    "#    A smaller λ allows the model to have lower bias but potentially higher variance, leading to overfitting.\n",
    "#    A larger λ introduces higher bias by shrinking coefficients, leading to a simpler model with lower variance and potentially improved generalization.\n",
    "\n",
    "# It's important to note that the choice of λ is problem-specific and requires careful evaluation. A well-chosen λ can help you find the right balance between model complexity and predictive accuracy, resulting in a model that performs well on new, unseen data.\n",
    "\n",
    "# Other than λ, Lasso Regression does not have additional tuning parameters like some other techniques (e.g., Ridge Regression has λ and degree of polynomial features). The regularization parameter λ is the primary tool to control the behavior of the Lasso model and affect the selection and magnitude of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf2658-f3d5-41d8-aaee-eebd69fe899f",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c5fc2-1473-47c7-bd9b-549705aa07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression is primarily designed for linear regression problems, where the relationship between predictor variables and the response variable is assumed to be linear. However, it can be extended to handle non-linear regression problems by incorporating transformations of the original features or by using polynomial features. Here's how you can adapt Lasso Regression for non-linear regression:\n",
    "\n",
    "#    Polynomial Features:\n",
    "#    One way to handle non-linear relationships is to introduce polynomial features. Transform your original predictor variables into polynomial terms (e.g., x2x2, x3x3) and then apply Lasso Regression. This approach allows the model to capture non-linear patterns by fitting a polynomial curve to the data.\n",
    "\n",
    "#    Feature Transformations:\n",
    "#    Besides polynomial features, you can use other non-linear transformations of the features, such as logarithmic, exponential, or trigonometric functions. These transformations can help the model capture complex non-linear relationships.\n",
    "\n",
    "#    Interaction Terms:\n",
    "#    Interaction terms involve multiplying different predictor variables together. They can help capture interactions between features, which might be necessary for certain non-linear relationships.\n",
    "\n",
    "#    Domain Knowledge:\n",
    "#    Prior knowledge of the problem domain can guide the selection of appropriate non-linear transformations. Domain expertise can provide insights into which transformations are likely to be relevant.\n",
    "\n",
    "#    Regularization Strength:\n",
    "#    When applying Lasso Regression to non-linear problems, the choice of the regularization parameter (λ) becomes even more important. You need to balance the complexity of the non-linear model with the need for regularization to prevent overfitting.\n",
    "\n",
    "#    Feature Engineering:\n",
    "#    Effective feature engineering is crucial when extending Lasso Regression to non-linear problems. Experiment with different transformations and combinations of features to find the best representation of non-linear relationships.\n",
    "\n",
    "#    Cross-Validation:\n",
    "#    Cross-validation is essential for selecting an appropriate λλ value and evaluating the model's performance. It ensures that the non-linear model generalizes well to new data.\n",
    "\n",
    "# It's important to note that while Lasso Regression can handle non-linear relationships to some extent through feature engineering and transformations, it might not capture highly complex non-linear patterns as effectively as more advanced non-linear regression techniques. If the underlying relationship is highly non-linear, techniques like decision trees, random forests, support vector regression, or neural networks might be more suitable choices. Nonetheless, Lasso Regression with non-linear features can still be useful when you want to balance predictive accuracy with model simplicity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91251f-4658-4e26-91b9-62887ad3fc6d",
   "metadata": {},
   "source": [
    "### Question6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203262c5-7896-4790-a14f-7c3dda3bb530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression and Lasso Regression are both regularized linear regression techniques that introduce penalties to the ordinary least squares (OLS) cost function. They share the goal of preventing overfitting and improving the stability of regression models, but they differ in how they apply the regularization and the impact on the coefficients. Here are the key differences between Ridge and Lasso Regression:\n",
    "\n",
    "#    Type of Penalty:\n",
    "#        Ridge Regression: It applies an L2 regularization penalty, which adds the sum of squared coefficients to the cost function: λ∑i=1 to pβi^2. This encourages small but non-zero coefficients for all features.\n",
    "#        Lasso Regression: It applies an L1 regularization penalty, which adds the absolute sum of coefficients to the cost function: λ∑i=1 to p∣βi∣ This encourages some coefficients to become exactly zero, effectively performing feature selection.\n",
    "\n",
    "#    Feature Selection:\n",
    "#        Ridge Regression: It does not lead to exact zero coefficients. It shrinks all coefficients towards zero but retains all features in the model.\n",
    "#        Lasso Regression: It can lead to exact zero coefficients, effectively excluding some features from the model. This makes Lasso Regression inherently suitable for feature selection.\n",
    "\n",
    "#    Impact on Coefficients:\n",
    "#        Ridge Regression: It reduces the magnitude of coefficients but does not force them to be zero. It helps to manage multicollinearity and prevent overfitting by making coefficients smaller.\n",
    "#        Lasso Regression: It reduces the magnitude of coefficients and forces some coefficients to be exactly zero. It provides a sparse model where only a subset of features contributes to the predictions.\n",
    "\n",
    "#    Model Complexity:\n",
    "#        Ridge Regression: It can lead to models with many non-zero coefficients, especially when there are correlated features. The regularization tends to shrink coefficients towards each other.\n",
    "#        Lasso Regression: It can lead to models with a smaller number of non-zero coefficients, resulting in a simpler and more interpretable model.\n",
    "\n",
    "#    Multicollinearity Handling:\n",
    "#        Ridge Regression: It effectively handles multicollinearity by distributing the impact of correlated features across multiple coefficients.\n",
    "#        Lasso Regression: It can handle multicollinearity by selecting one feature from a correlated group and setting others to zero. This can simplify the model and improve interpretability.\n",
    "\n",
    "#    Scaling Sensitivity:\n",
    "#        Ridge Regression: It is less sensitive to the scale of predictor variables due to the squared term in the penalty.\n",
    "#        Lasso Regression: It can be sensitive to the scale of predictor variables, so proper scaling is important.\n",
    "\n",
    "#    Regularization Parameter Selection:\n",
    "#        Ridge Regression: The optimal λ value balances between bias and variance, and cross-validation can help determine it.\n",
    "#        Lasso Regression: The choice of λ affects the sparsity of the model, and cross-validation assists in selecting the appropriate λλ that performs feature selection effectively.\n",
    "\n",
    "#In summary, Ridge Regression and Lasso Regression differ in the type of regularization penalty they apply and the impact on coefficients. Ridge is useful for handling multicollinearity and reducing the magnitude of coefficients, while Lasso is particularly effective for feature selection and generating sparse models. The choice between Ridge and Lasso depends on the specific characteristics of your data and your goals for model complexity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32780bde-7557-448f-af01-eb6d929886fb",
   "metadata": {},
   "source": [
    "### Question7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a474c5-88cf-4f72-9e15-cded59d71b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, Lasso Regression can handle multicollinearity in the input features, although it does so in a different way compared to Ridge Regression. Multicollinearity occurs when predictor variables are highly correlated with each other, which can lead to instability in coefficient estimates in ordinary least squares (OLS) regression. Lasso Regression addresses multicollinearity by performing feature selection and shrinking coefficients towards zero, effectively reducing the impact of correlated features. Here's how Lasso handles multicollinearity:\n",
    "\n",
    "#    Feature Selection:\n",
    "#    Lasso Regression's L1 regularization penalty encourages some coefficients to become exactly zero. When two or more features are highly correlated, Lasso is likely to select one of them and set the others to zero. This means that Lasso automatically chooses the most relevant feature from a group of correlated variables, effectively performing implicit feature selection.\n",
    "\n",
    "#    Sparse Models:\n",
    "#    By setting coefficients of some correlated variables to zero, Lasso creates a sparse model with fewer active predictor variables. This can lead to a simpler and more interpretable model while addressing the multicollinearity issue.\n",
    "\n",
    "#    Coefficients Distribution:\n",
    "#    Lasso's regularization spreads the contribution of correlated features across multiple coefficients. While Ridge Regression distributes the impact more evenly, Lasso tends to assign more weight to one of the correlated features and less weight to the others.\n",
    "\n",
    "#    Impact on Non-Correlated Features:\n",
    "#    Lasso can also shrink coefficients of non-correlated features towards zero, depending on the value of the regularization parameter (λ). This helps prevent overfitting and reduces the risk of inflated coefficient estimates.\n",
    "\n",
    "#    Regularization Parameter (λ) Selection:\n",
    "#    The choice of λ in Lasso Regression is crucial. A higher λ emphasizes stronger regularization, which can lead to more coefficients becoming zero, effectively handling multicollinearity. Proper cross-validation is essential to select an optimal λλ that balances model complexity and predictive performance.\n",
    "\n",
    "#    Impact on Model Complexity:\n",
    "#    Lasso's ability to select one variable from a correlated group can simplify the model and improve interpretability, but it might not completely eliminate multicollinearity. Some degree of multicollinearity might still exist if features are strongly correlated.\n",
    "\n",
    "#    Scale Sensitivity:\n",
    "#    Lasso can be sensitive to the scale of predictor variables, so it's important to standardize or normalize the variables before applying Lasso Regression to ensure fair regularization across all features.\n",
    "\n",
    "# In summary, Lasso Regression handles multicollinearity by performing implicit feature selection and shrinking coefficients towards zero. It automatically chooses relevant features from correlated groups, leading to a sparse model that can mitigate the issues caused by multicollinearity and produce more reliable coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f037338-33b5-45ff-870c-9476c54fb0fd",
   "metadata": {},
   "source": [
    "### Question8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf8c33-bb58-4064-b592-72308c284953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the optimal value of the regularization parameter (λ) in Lasso Regression is a critical step to balance between model complexity and predictive performance. The optimal λ value influences the sparsity of the model and the number of coefficients that become exactly zero. Several techniques can help you select the best λ value:\n",
    "\n",
    "#    Grid Search with Cross-Validation:\n",
    "#    One common approach is to perform a grid search over a range of λ values. For each λ value, use cross-validation to assess the model's performance on a validation set. Choose the λ that provides the best trade-off between bias and variance, typically yielding the lowest validation error.\n",
    "\n",
    "#    K-Fold Cross-Validation:\n",
    "#    Implement k-fold cross-validation, where the dataset is divided into k subsets (folds). For each λ value, train the Lasso Regression model on k-1 folds and evaluate it on the remaining fold. Repeat this process for each fold and calculate the average performance metric (e.g., mean squared error). The λ value with the lowest average performance metric is the optimal choice.\n",
    "\n",
    "#    Leave-One-Out Cross-Validation (LOOCV):\n",
    "#    LOOCV is a form of cross-validation where each observation is used as a validation set, and the model is trained on the remaining data. It provides a robust assessment of model performance for each λ value. However, LOOCV can be computationally expensive, especially for large datasets.\n",
    "\n",
    "#    Information Criteria:\n",
    "#    Information criteria like AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) can also guide λ selection. These criteria aim to balance model fit with model complexity. A lower AIC or BIC value indicates a better trade-off.\n",
    "\n",
    "#    Regularization Path:\n",
    "#    Plot the regularization path, which shows how the coefficients change as λ varies. This can provide insights into which features become non-zero or zero at different λ values.\n",
    "\n",
    "#    Coordinate Descent Algorithm:\n",
    "#    Some software packages (like scikit-learn in Python) implement algorithms that can automatically determine an optimal λ using efficient techniques like coordinate descent.\n",
    "\n",
    "#    Cross-Validation for Specific Goals:\n",
    "#    Consider the specific goals of your analysis. For example, if interpretability is crucial, choose the λ value that results in a model with a small number of non-zero coefficients.\n",
    "\n",
    "# Remember that the optimal λ value might vary depending on the dataset and the problem at hand. Cross-validation helps you select λ based on how well the model generalizes to unseen data. Be cautious not to overfit to the validation set while selecting λ, as the model's performance on new, independent data is the ultimate goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

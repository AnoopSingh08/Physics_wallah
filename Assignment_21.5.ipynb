{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ad352b-a5d0-4342-bc11-e9e459d1abbf",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d2b2f-7543-4ac2-a034-f01e165dd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow:\n",
    "\n",
    "# You can install TensorFlow using pip, a Python package manager. Open your terminal or command prompt and run the following command:\n",
    "\n",
    "# pip install tensorflow\n",
    "\n",
    "# Install Keras:\n",
    "\n",
    "# Keras is now integrated with TensorFlow as the high-level API, so you don't need to install it separately. When you install TensorFlow, you'll also get Keras as part of the package.\n",
    "\n",
    "# Check TensorFlow Version:\n",
    "\n",
    "# You can check the TensorFlow version in Python using the following code:\n",
    "\n",
    "#     import tensorflow as tf\n",
    "\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check Keras Version:\n",
    "\n",
    "# Since Keras is integrated with TensorFlow, you can check the Keras version as follows:\n",
    "\n",
    "\n",
    "#     import tensorflow as tf\n",
    "\n",
    "#     print(\"Keras version:\", tf.keras.__version__)\n",
    "\n",
    "# Please note that these instructions are based on the assumption that TensorFlow and Keras are still integrated. If there have been significant changes or if you prefer to use a separate version of Keras, you can install it explicitly using pip. Always refer to the official documentation and release notes for the most up-to-date information on software versions and installation procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0002a9-8a25-44d4-8a17-086d0c9e70d0",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312136f4-414e-446e-a73d-64107c861af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the steps you can follow to load and explore the dataset:\n",
    "\n",
    "#     Download the Dataset:\n",
    "#         Download the Wine Quality dataset from the provided Kaggle link to your local machine.\n",
    "\n",
    "#     Load the Dataset:\n",
    "\n",
    "#         You can load the dataset using Python and popular libraries such as Pandas. Make sure you have Pandas installed; if not, you can install it using pip:\n",
    "\n",
    "# pip install pandas\n",
    "\n",
    "# Once Pandas is installed, you can use the following code to load the dataset into a Pandas DataFrame:\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Replace 'winequality-binary-classification.csv' with the actual file path if needed\n",
    "    df = pd.read_csv('winequality-binary-classification.csv')\n",
    "\n",
    "    # Display the first few rows of the dataset to inspect its structure\n",
    "    print(df.head())\n",
    "\n",
    "# Explore the Dimensions:\n",
    "\n",
    "#     To explore the dimensions of the dataset, you can use the shape attribute of the DataFrame, which will provide you with the number of rows and columns:\n",
    "\n",
    "\n",
    "        # Display the dimensions (number of rows and columns) of the dataset\n",
    "        print(\"Dimensions of the dataset:\", df.shape)\n",
    "\n",
    "#         This will print the number of rows (samples) and columns (features) in the Wine Quality dataset.\n",
    "\n",
    "# By following these steps, you should be able to load the Wine Quality dataset from your local machine and explore its dimensions using Python and Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721541e-5a7a-4498-b516-208010925fb2",
   "metadata": {},
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e061bc-e059-469d-a8bf-96c6de5c0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Null Values:\n",
    "\n",
    "#     To check for null (missing) values in the dataset, you can use the isnull() method along with the sum() function to count the number of missing values in each column:\n",
    "\n",
    "    # Check for null values in each column\n",
    "    null_counts = df.isnull().sum()\n",
    "    print(\"Null value counts:\\n\", null_counts)\n",
    "\n",
    "#     This will display the count of null values in each column of the DataFrame. If there are any null values, you may need to decide how to handle them (e.g., by imputing missing values or removing rows/columns with missing data).\n",
    "\n",
    "# Identify Categorical Variables:\n",
    "\n",
    "#     To identify categorical variables in the dataset, you can check the data types of the columns. Categorical variables are typically of data type 'object' or 'category' in Pandas.\n",
    "\n",
    "\n",
    "    # Identify categorical variables\n",
    "    categorical_vars = df.select_dtypes(include=['object', 'category']).columns\n",
    "    print(\"Categorical variables:\\n\", categorical_vars)\n",
    "\n",
    "#     This will display the names of columns that are considered categorical based on their data types.\n",
    "\n",
    "# Encode Categorical Variables:\n",
    "\n",
    "#     To encode categorical variables, you can use one-hot encoding, which converts categorical variables into binary columns (0 or 1) for each category. You can achieve this using the pd.get_dummies() function in Pandas.\n",
    "\n",
    "\n",
    "        # Perform one-hot encoding for categorical variables\n",
    "        df_encoded = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "#         In the code above, drop_first=True is used to drop the first category for each encoded variable to avoid multicollinearity.\n",
    "\n",
    "# Now, df_encoded will contain the Wine Quality dataset with categorical variables one-hot encoded, and you can use it for further analysis or machine learning tasks. Make sure to adjust the encoding and handling of missing values as needed for your specific analysis or modeling objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1e249-4d86-404b-a3ef-3bbc2633163b",
   "metadata": {},
   "source": [
    "### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd6f17-b0d6-4c9d-996a-d031cbe78fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate the features (independent variables) from the target variable (dependent variable) in your dataset, you can use Python and Pandas. Assuming you have loaded the Wine Quality dataset into a Pandas DataFrame called df, here's how you can perform this separation:\n",
    "\n",
    "\n",
    "# Assuming the target variable is 'target_column_name'\n",
    "# Replace 'target_column_name' with the actual name of your target column\n",
    "target_column_name = 'target_column_name'\n",
    "\n",
    "# Separate the features (X) and target (y) variables\n",
    "X = df.drop(columns=[target_column_name])  # X contains all columns except the target\n",
    "y = df[target_column_name]  # y contains only the target column\n",
    "\n",
    "# Optionally, you can convert y to a NumPy array if needed\n",
    "import numpy as np\n",
    "y = np.array(y)\n",
    "\n",
    "# Verify the shapes of X and y\n",
    "print(\"Shape of X:\", X.shape)  # This will show the number of samples and features in X\n",
    "print(\"Shape of y:\", y.shape)  # This will show the number of samples in y\n",
    "\n",
    "# In the code above:\n",
    "\n",
    "#     X contains all the columns from the DataFrame df except for the target column specified by target_column_name. It represents the feature variables.\n",
    "\n",
    "#     y contains only the target column specified by target_column_name. It represents the target variable.\n",
    "\n",
    "#     If you need to use y as a NumPy array (e.g., for machine learning models), you can convert it using np.array(y).\n",
    "\n",
    "# Please replace 'target_column_name' with the actual name of the target variable column in your dataset. After executing this code, you will have separated the features and target variable, which you can use for further analysis or machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ccfb9a-e872-44f7-ad0d-528a2ce92602",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7682bb8-e8de-44a3-8fa5-9ccee313f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a train-test split, including creating a validation set, is essential for training and evaluating machine learning models. You can use the train_test_split function from Scikit-Learn to accomplish this. Here's how to split your data into training, validation, and test datasets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the temporary data into validation and test sets\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"Training set - X:\", X_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Validation set - X:\", X_valid.shape, \"y:\", y_valid.shape)\n",
    "print(\"Test set - X:\", X_test.shape, \"y:\", y_test.shape)\n",
    "\n",
    "# In the code above:\n",
    "\n",
    "#     X and y are your feature and target variables, respectively.\n",
    "\n",
    "#     We first perform a split into a training set (X_train and y_train) and temporary data (X_temp and y_temp) using test_size=0.3. This means that 30% of the data will be used for validation and testing combined.\n",
    "\n",
    "#     Then, we further split the temporary data into a validation set (X_valid and y_valid) and a test set (X_test and y_test) using test_size=0.5. This means that 15% of the original data will be used for validation and 15% for testing.\n",
    "\n",
    "#     The random_state parameter is set to ensure reproducibility. You can change the value or omit it if you prefer a different random split.\n",
    "\n",
    "# After executing this code, you will have three separate datasets: training, validation, and test sets, which you can use for training, tuning hyperparameters, and evaluating your machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef2bb4-6379-43b9-a025-213da737bca1",
   "metadata": {},
   "source": [
    "### Question6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94630de1-ec3a-47b6-b25d-e5d1999dc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the dataset is an essential preprocessing step, especially when you're working with features that have different scales or units. Common scaling techniques include Standardization (Z-score scaling) and Min-Max scaling. You can use the StandardScaler or MinMaxScaler from Scikit-Learn to perform these scaling operations. Here's how to scale your dataset using Standardization:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test data using the same scaler\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Now, X_train_scaled, X_valid_scaled, and X_test_scaled are the scaled feature sets\n",
    "\n",
    "# In the code above:\n",
    "\n",
    "#     We initialize a StandardScaler object.\n",
    "\n",
    "#     We fit the scaler to the training data (X_train) using the .fit_transform() method. This calculates the mean and standard deviation of each feature in the training set and scales the data accordingly.\n",
    "\n",
    "#     We then transform the validation and test data (X_valid and X_test) using the same scaler. It's important to use the same scaling parameters (mean and standard deviation) calculated from the training data to ensure consistency.\n",
    "\n",
    "# After these operations, X_train_scaled, X_valid_scaled, and X_test_scaled will contain the scaled feature sets, which can be used for training and evaluating machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b33fcc-3ed6-41e2-9000-a3c52c5ebfa9",
   "metadata": {},
   "source": [
    "### Question7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d68a7-ea37-435c-ae52-d6d14a7ad569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To design and implement a neural network with at least two hidden layers and an output layer for binary categorical variables, you can use a deep learning framework like TensorFlow and Keras. Below is an example of how to create a neural network with two hidden layers and an output layer for binary classification:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    # Input layer (specify input shape based on the number of features)\n",
    "    keras.layers.Input(shape=(num_features,)),\n",
    "    \n",
    "    # First hidden layer with 128 units and ReLU activation\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # Second hidden layer with 64 units and ReLU activation\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    # Output layer with 1 unit and sigmoid activation for binary classification\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # You can choose a different optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# In the code above:\n",
    "\n",
    "#     We create a Sequential model, which allows you to stack layers one after the other.\n",
    "\n",
    "#     The input layer is implicitly defined when you specify the input_shape, where num_features should be replaced with the number of input features in your dataset.\n",
    "\n",
    "#     Two hidden layers are added with 128 and 64 units, respectively. You can adjust the number of units according to your model's complexity requirements.\n",
    "\n",
    "#     The activation function used in the hidden layers is ReLU (Rectified Linear Unit), which is a common choice for hidden layers.\n",
    "\n",
    "#     The output layer has a single unit with a sigmoid activation function, which is suitable for binary classification problems. If you have more than two classes, you can use a different activation function like softmax.\n",
    "\n",
    "#     The model is compiled with the Adam optimizer, binary cross-entropy loss (appropriate for binary classification), and accuracy as the evaluation metric.\n",
    "\n",
    "#     Finally, we print the model summary, which provides an overview of the architecture and the number of trainable parameters.\n",
    "\n",
    "# You can further customize the architecture, hyperparameters, and training process based on your specific dataset and problem requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b61c3-9099-4d64-9745-7c27637a6451",
   "metadata": {},
   "source": [
    "### Question8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a7737-6d64-44f1-b2ed-3eeebdb644b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a Sequential model in Keras and add the previously designed layers to it, you can follow these steps. Make sure you've defined the layers as shown in the previous response. Here's how to create the Sequential model and add the layers:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Initialize a Sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add the layers to the model\n",
    "model.add(keras.layers.Input(shape=(num_features,)))  # Input layer\n",
    "model.add(keras.layers.Dense(128, activation='relu'))  # First hidden layer\n",
    "model.add(keras.layers.Dense(64, activation='relu'))   # Second hidden layer\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # You can choose a different optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We initialize a Sequential model by creating an instance of keras.Sequential().\n",
    "\n",
    "#     We add the layers to the model using the model.add() method. The layers are added in the order they appear in the code: input layer, first hidden layer, second hidden layer, and output layer.\n",
    "\n",
    "#     The input layer is specified using keras.layers.Input(shape=(num_features,)).\n",
    "\n",
    "#     The two hidden layers and the output layer are added using keras.layers.Dense(). You can customize the number of units and activation functions as needed.\n",
    "\n",
    "#     The compile() method is used to compile the model with an optimizer, loss function, and evaluation metric.\n",
    "\n",
    "#     Finally, we print the model summary to get an overview of the model's architecture and the number of trainable parameters.\n",
    "\n",
    "# This Sequential model is now ready for training and evaluation. You can adjust the architecture and hyperparameters as needed for your specific problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc49724-f8a9-4db9-9abb-9031db071cec",
   "metadata": {},
   "source": [
    "#### Question9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9afaa-cc0d-4084-a9b8-d269069178aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can print the summary of the model architecture using the summary() method of the Keras model. Here's how to do it:\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# After you've created the model and added the layers as previously described, executing the model.summary() command will display a summary of the model's architecture, including details about the layers, the number of trainable parameters, and the output shapes. This summary provides a helpful overview of your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74aabdb-d35a-4495-a098-8016975193ff",
   "metadata": {},
   "source": [
    "### Question10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637cbff5-0b00-4098-87ae-8b84cbb523d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You've already set the loss function to 'binary_crossentropy' and the optimizer to 'adam' when compiling the model. However, you can include the accuracy metric in the model explicitly using the metrics argument. Here's how to do it:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Initialize a Sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add the layers to the model\n",
    "model.add(keras.layers.Input(shape=(num_features,)))  # Input layer\n",
    "model.add(keras.layers.Dense(128, activation='relu'))  # First hidden layer\n",
    "model.add(keras.layers.Dense(64, activation='relu'))   # Second hidden layer\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# Compile the model with loss, optimizer, and metrics\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])  # Include accuracy as a metric\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# In the code above:\n",
    "\n",
    "#     We've included the accuracy metric by specifying metrics=['accuracy'] when compiling the model. This means that during training and evaluation, the model will compute and report the accuracy metric as well as the loss.\n",
    "\n",
    "#     The loss function remains 'binary_crossentropy,' and the optimizer is 'adam,' as specified in your previous code.\n",
    "\n",
    "# Now, your Keras model is set to use binary cross-entropy as the loss function, the 'adam' optimizer for training, and it will calculate and display accuracy as a metric during training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68487e8d-337f-4335-b11b-af40ca1bf673",
   "metadata": {},
   "source": [
    "#### Question11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27dd13-b34f-4e99-b656-142fae0c5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You've already compiled the model with the specified loss function ('binary_crossentropy'), optimizer ('adam'), and metrics (including 'accuracy') in the previous code. Here's the code snippet for reference:\n",
    "\n",
    "# Compile the model with loss, optimizer, and metrics\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])  # Include accuracy as a metric\n",
    "\n",
    "# In this code, the model is compiled with the loss function set to 'binary_crossentropy,' the optimizer set to 'adam,' and the accuracy metric included for monitoring during training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0a727-57d0-4d57-bd71-941e6be54bff",
   "metadata": {},
   "source": [
    "### Question12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f66de8-7c1b-40cc-aa2e-bf5020a532cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fit the model to the training data, you need to specify the appropriate batch size and the number of epochs for training. The batch size determines how many samples are used in each iteration of training, while the number of epochs defines how many times the entire training dataset is passed through the network. Here's how to fit the model with suitable values for batch size and epochs:\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 32  # You can adjust this value based on your dataset and hardware\n",
    "epochs = 10      # You can adjust the number of epochs based on your training needs\n",
    "\n",
    "# Fit the model to the training data\n",
    "history = model.fit(X_train_scaled,  # Training features\n",
    "                    y_train,         # Training labels\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))  # Validation data\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     batch_size is set to 32, which is a common starting point. You can adjust this value depending on your dataset size and hardware capabilities. Smaller batch sizes consume less memory but may result in slower convergence, while larger batch sizes can lead to faster training but require more memory.\n",
    "\n",
    "#     epochs is set to 10 as an example. You can increase or decrease this value based on your training requirements. Training for more epochs may improve model performance, but it can also lead to overfitting if not properly monitored.\n",
    "\n",
    "#     The model.fit() method is used to train the model. It takes the training features (X_train_scaled) and labels (y_train) as input, along with the batch size, number of epochs, and optional validation data ((X_valid_scaled, y_valid)) to monitor the model's performance during training.\n",
    "\n",
    "#     The training history is stored in the history variable, which contains information about the training and validation loss and accuracy for each epoch.\n",
    "\n",
    "# You can adjust the batch size and number of epochs based on your specific dataset and training objectives. Monitoring the training process through the history object will help you assess model performance and make informed decisions about training duration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd50901-818e-48be-a10c-f71c912936de",
   "metadata": {},
   "source": [
    "#### Question13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48120216-6e06-40da-8d0a-7f4f01f988c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can obtain the model's parameters (weights and biases) using the get_weights() method for each layer of the model. Here's how you can retrieve the weights and biases:\n",
    "\n",
    "# Get the weights and biases for each layer in the model\n",
    "all_weights = []\n",
    "all_biases = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    weights, biases = layer.get_weights()\n",
    "    all_weights.append(weights)\n",
    "    all_biases.append(biases)\n",
    "\n",
    "# Print the weights and biases for each layer\n",
    "for i, (weights, biases) in enumerate(zip(all_weights, all_biases)):\n",
    "    print(f\"Layer {i+1}:\")\n",
    "    print(f\"Weights shape: {weights.shape}\")\n",
    "    print(f\"Biases shape: {biases.shape}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We iterate through each layer in the model using a for loop.\n",
    "\n",
    "#     For each layer, we use the get_weights() method to retrieve the weights and biases. The get_weights() method returns a list where the first element is the weights matrix and the second element is the biases vector.\n",
    "\n",
    "#     We store the weights and biases in separate lists (all_weights and all_biases) for further inspection or analysis.\n",
    "\n",
    "#     Finally, we print the shapes of the weights and biases for each layer to examine their dimensions.\n",
    "\n",
    "# This code allows you to access and inspect the model's parameters, which can be useful for various purposes, such as visualizing learned features, performing model interpretation, or saving and loading trained models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57ff1d-20a7-43b7-9349-ccc23f672272",
   "metadata": {},
   "source": [
    "#### Question14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c4595-a0e0-4337-84ab-8d91c8c769ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can store the model's training history as a Pandas DataFrame to easily analyze and visualize the training metrics. Here's how you can do it:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the training history to a Pandas DataFrame\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(history_df.head())\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We use pd.DataFrame(history.history) to convert the training history (typically stored in the history object) to a Pandas DataFrame.\n",
    "\n",
    "#     The resulting history_df DataFrame will contain columns for training loss, training accuracy, validation loss, and validation accuracy for each epoch.\n",
    "\n",
    "#     You can now use this DataFrame for further analysis, plotting training curves, or saving the training history to a file for future reference.\n",
    "\n",
    "# Keep in mind that the structure of the history object may vary slightly depending on the deep learning framework you are using. However, it typically contains metrics such as loss and accuracy for each epoch, which can be easily converted to a DataFrame as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a435c33-2127-4f50-b40c-0fe660b6f24e",
   "metadata": {},
   "source": [
    "#### Question15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36fc2a-d8c8-48dc-8855-4ea9534338d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the training history, you can use suitable visualization techniques. Commonly, accuracy and loss are visualized using line plots. You can achieve this using Python libraries like Matplotlib. Here's how to plot the training history:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access the training history from the Pandas DataFrame\n",
    "training_loss = history_df['loss']\n",
    "training_accuracy = history_df['accuracy']\n",
    "validation_loss = history_df['val_loss']\n",
    "validation_accuracy = history_df['val_accuracy']\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Create subplots for accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, training_accuracy, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracy, 'ro-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We first access the training history metrics (loss and accuracy) from the Pandas DataFrame created earlier.\n",
    "\n",
    "#     We define the number of epochs to create the x-axis for plotting.\n",
    "\n",
    "#     We create subplots to display accuracy and loss side by side.\n",
    "\n",
    "#     We use Matplotlib to plot training and validation loss in one subplot and training and validation accuracy in the other subplot.\n",
    "\n",
    "#     The blue markers represent training data, and the red markers represent validation data.\n",
    "\n",
    "#     We set titles, labels, and legends to make the plots more informative.\n",
    "\n",
    "#     Finally, we display the plots using plt.show().\n",
    "\n",
    "# Running this code will generate two subplots: one showing training and validation loss and the other showing training and validation accuracy across epochs. These visualizations help you monitor the training progress and identify any overfitting or underfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03e888-f169-4a9d-8848-0f439fe998df",
   "metadata": {},
   "source": [
    "#### Question16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca103f5f-ed8f-418c-afc9-aa6bed3a9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate the model's performance using the test dataset and report relevant metrics (such as accuracy, precision, recall, F1-score, and confusion matrix), you can use Scikit-Learn's classification_report and confusion_matrix functions. Here's how to do it:\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict using the model on the test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predicted probabilities to binary labels (0 or 1)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "# Generate a classification report\n",
    "class_report = classification_report(y_test, y_pred_binary)\n",
    "\n",
    "# Print the confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We use the trained model to make predictions on the test data (X_test_scaled) and obtain predicted probabilities.\n",
    "\n",
    "#     We convert the predicted probabilities to binary labels by thresholding at 0.5 (you can adjust this threshold if needed).\n",
    "\n",
    "#     The confusion_matrix function computes the confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "#     The classification_report function generates a report containing precision, recall, F1-score, and support (number of instances) for each class (0 and 1).\n",
    "\n",
    "#     Finally, we print the confusion matrix and classification report to assess the model's performance on the test dataset.\n",
    "\n",
    "# These metrics provide valuable insights into how well the model is performing in terms of binary classification. Adjust the threshold or consider additional metrics depending on your specific problem and requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

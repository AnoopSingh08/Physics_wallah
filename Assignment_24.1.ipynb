{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d5f0b0-c150-4c5c-b432-7fa76a44842a",
   "metadata": {},
   "source": [
    "#### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a5cfe-04f0-4190-89ec-bf06d60fa9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sure, training a Generative Adversarial Network (GAN) using the CIFAR-10 dataset is a common and interesting task in the field of deep learning. However, please note that generating high-quality images might require substantial computational resources and time. Here's an outline of how you might approach this:\n",
    "\n",
    "# Preparation:\n",
    "# CIFAR-10 Dataset: Download the CIFAR-10 dataset. It contains 60,000 32x32 color images in 10 different classes.\n",
    "# Deep Learning Framework: You can use a framework like TensorFlow or PyTorch to build and train the GAN model.\n",
    "# Building the GAN:\n",
    "# Generator Network: This network generates fake images. It typically consists of deconvolutional layers (inverting convolutions) to map random noise to images.\n",
    "# Discriminator Network: This network discriminates between real and fake images. It's a typical CNN that classifies images as real or fake.\n",
    "# Training Loop: The training involves optimizing the generator and discriminator networks in an adversarial manner.\n",
    "# Training Steps:\n",
    "# Data Preprocessing: Prepare the CIFAR-10 dataset, normalize the images, and set up your data loaders.\n",
    "# Model Initialization: Initialize the generator and discriminator networks.\n",
    "# Loss Functions: Define loss functions for the generator and discriminator.\n",
    "# Training Loop: Alternately train the generator and discriminator by passing real and fake images.\n",
    "# Generating Images:\n",
    "# Once the GAN is trained, you can use the trained generator to create new images by passing random noise through the generator network.\n",
    "\n",
    "# Code Example (using PyTorch):\n",
    "# Below is a simplified PyTorch code snippet to demonstrate the training process:\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the Generator and Discriminator networks (define these models or load pre-defined architectures)\n",
    "\n",
    "# Define the loss function and optimizers\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Training the discriminator\n",
    "\n",
    "        # Training the generator\n",
    "\n",
    "# Generating new images\n",
    "num_images_to_generate = 10\n",
    "noise = torch.randn(num_images_to_generate, 100)  # Generate random noise\n",
    "fake_images = generator(noise)  # Generate images using the trained generator\n",
    "# Display or save these generated images\n",
    "# Please note that this is a simplified example. Building and training GAN models often require more complex architectures and careful tuning of hyperparameters to generate high-quality images. Also, it's essential to have access to powerful GPUs or TPUs for training GANs effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3ea50-be32-4f8e-bb7f-a0d478f389da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Generative Adversarial Network (GAN), monitoring the generator and discriminator losses during training is a common practice to observe the convergence of the model. The convergence of GANs is usually assessed through the change in losses and the visual quality of generated images.\n",
    "\n",
    "# Below is an example of how you might visualize the generator and discriminator losses during training using Python and Matplotlib.\n",
    "\n",
    "# Assuming you have lists or arrays storing the generator and discriminator losses during training, you can create a plot to visualize these losses. Here's an example:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have lists storing generator and discriminator losses (change these to your actual lists)\n",
    "generator_losses = [0.5, 0.3, 0.1, 0.08, 0.06]\n",
    "discriminator_losses = [1.2, 0.9, 0.5, 0.3, 0.2]\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(generator_losses, label='Generator Loss', color='blue')\n",
    "plt.plot(discriminator_losses, label='Discriminator Loss', color='orange')\n",
    "plt.title('Generator and Discriminator Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# In this example, generator_losses and discriminator_losses are placeholder lists. You should replace them with your actual training data containing the losses. By plotting these losses across epochs, you can visualize how they change during the training process.\n",
    "\n",
    "# Understanding convergence in GANs:\n",
    "\n",
    "# Generator Loss: The generator loss tends to decrease over time. It represents how well the generator is fooling the discriminator. A decreasing generator loss indicates that the generator is improving in generating more realistic images.\n",
    "\n",
    "# Discriminator Loss: The discriminator loss should decrease as well. This shows how well the discriminator can differentiate between real and fake images. A decreasing discriminator loss means the discriminator is getting better at distinguishing real from generated images.\n",
    "\n",
    "# Ascertaining convergence in GANs:\n",
    "\n",
    "# Stable and Decreasing Losses: Convergence is often achieved when both the generator and discriminator losses stabilize and reach a point where they're no longer decreasing significantly.\n",
    "# Visual Inspection of Generated Images: Alongside loss plots, visual inspection of generated images can help ascertain convergence. As the model converges, the generated images should become more realistic and closer in appearance to the real images from the dataset.\n",
    "# Keep in mind that achieving convergence in GANs can sometimes be challenging and might require fine-tuning of hyperparameters, architectures, and training strategies. Also, these plots serve as an indication, but further analysis might be necessary to confirm the model's convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782aac2-e074-474f-88f3-7189fb4d94b5",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfeffd9-86c0-4591-a208-b29e8e201564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certainly, here's a step-by-step guide for achieving the outlined tasks using the ResNet50 model with CIFAR-10 dataset:\n",
    "\n",
    "# A. Training the Complete Network from Scratch\n",
    "# Load CIFAR-10 Dataset\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset.\n",
    "# Prepare the ResNet50 Model without Classification Layer\n",
    "\n",
    "# Load the ResNet50 model pre-trained on ImageNet without the top classification layer.\n",
    "# Replace the top layer with a 2-layer neural network followed by a Softmax layer.\n",
    "# Training from Scratch\n",
    "\n",
    "# Train the entire modified ResNet50 model (including the added layers) from scratch on the CIFAR-10 dataset.\n",
    "# Calculate classification accuracy on the train set and test set.\n",
    "# Plot accuracies over epochs.\n",
    "# B. Using Pre-trained ResNet50 Weights for Only the Neural Network Layers\n",
    "# Prepare the Model with Pre-trained Weights\n",
    "\n",
    "# Load the ResNet50 model pre-trained on ImageNet without the classification layer.\n",
    "# Replace the top layer with a 2-layer neural network followed by a Softmax layer.\n",
    "# Freeze ResNet50 Weights\n",
    "\n",
    "# Freeze the weights of the ResNet50 layers.\n",
    "# Train Only the Neural Network Layers\n",
    "\n",
    "# Train only the added neural network layers on the CIFAR-10 dataset.\n",
    "# Calculate classification accuracy on the train set and test set.\n",
    "# Plot accuracies over epochs.\n",
    "# C. Adapting All Layers of Pre-trained ResNet50\n",
    "# Prepare the Pre-trained ResNet50 Model\n",
    "\n",
    "# Load the ResNet50 model pre-trained on ImageNet without the classification layer.\n",
    "# Train All Layers\n",
    "\n",
    "# Unfreeze all layers of ResNet50.\n",
    "# Train the entire network on the CIFAR-10 dataset.\n",
    "# Calculate classification accuracy on the train set and test set.\n",
    "# Plot accuracies over epochs.\n",
    "# D. Domain Adaptation Algorithm\n",
    "# For domain adaptation, you can use techniques like domain adversarial training, self-training, or transfer learning with additional datasets. For example:\n",
    "\n",
    "# Domain Adversarial Training: Introduce an additional domain-adversarial loss to make the model invariant to domain shift. You might train the model on CIFAR-10 and then adapt it to another dataset.\n",
    "# Self-Training: Train the model on CIFAR-10 and use pseudo-labeled samples from another related dataset to further fine-tune the model.\n",
    "# Transfer Learning with Other Datasets: Utilize pre-trained models on other related datasets and fine-tune them on CIFAR-10.\n",
    "# The success of the domain adaptation algorithm relies on the similarity between the source domain (CIFAR-10) and the target domain (other datasets) and the effectiveness of the adaptation technique to handle domain shift while maintaining or enhancing the model's accuracy.\n",
    "\n",
    "# Please note that the performance improvement will depend on the specific domains used for adaptation and the efficacy of the adaptation method in addressing domain differences while preserving or enhancing the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5bf88-2d2a-4b1c-9cf7-c412e6ae3d16",
   "metadata": {},
   "source": [
    "#### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31cacfe-c43a-4910-94a9-7b6def24a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a GAN (Generative Adversarial Network) using Keras to generate celebrity faces from the CelebA dataset involves several steps. Please make sure you have the CelebA dataset downloaded and extracted.\n",
    "\n",
    "# Below is an example of how you can implement a basic GAN architecture for image generation using the CelebA dataset. We'll focus on generating human face images using Keras and TensorFlow.\n",
    "\n",
    "# First, ensure you have the necessary libraries installed:\n",
    "\n",
    "\n",
    "# pip install numpy pandas matplotlib keras tensorflow\n",
    "# Next, let's proceed with the GAN implementation:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "# Load and preprocess the CelebA dataset\n",
    "# You'll need to adjust this code based on how you've organized and preprocessed the CelebA dataset\n",
    "# For instance, you'll likely need to resize the images to a common size and normalize the pixel values.\n",
    "# Ensure your images are loaded and processed to a shape suitable for the generator input and discriminator output.\n",
    "\n",
    "# Define the generator\n",
    "def build_generator():\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(256, input_dim=100))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(Dense(32*32*3, activation='tanh'))  # Output layer for CelebA face images\n",
    "    generator.add(Reshape((32, 32, 3)))\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# Define the discriminator\n",
    "def build_discriminator():\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Flatten(input_shape=(32, 32, 3)))\n",
    "    discriminator.add(Dense(1024))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))  # Output layer, binary classification real/fake\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "# Compile the GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(100,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return gan\n",
    "\n",
    "# Training the GAN\n",
    "def train_gan(gan, generator, discriminator, epochs=100, batch_size=64):\n",
    "    # Load and preprocess the CelebA dataset\n",
    "    # Replace the following lines with your code to load and preprocess the CelebA images\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for _ in range(batch_size):\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            generated_images = generator.predict(noise)\n",
    "            image_batch = get_batch_of_images()  # Replace with code to get a batch of real images\n",
    "            \n",
    "            x = np.concatenate([image_batch, generated_images])\n",
    "            y_dis = np.zeros(2*batch_size)\n",
    "            y_dis[:batch_size] = 0.9  # Label smoothing for the real images\n",
    "            \n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(x, y_dis)\n",
    "            \n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            y_gen = np.ones(batch_size)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "        print(f'Epoch: {e}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}')\n",
    "\n",
    "# Initialize and train the GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "train_gan(gan, generator, discriminator, epochs=100, batch_size=64)\n",
    "# Please note that this code serves as a basic example and might need modifications to fit the CelebA dataset. Additionally, further adjustments and fine-tuning, such as hyperparameter tuning, might be required to improve the generated image quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b5cfab-2931-4b6f-8433-b7906a8496f0",
   "metadata": {},
   "source": [
    "##### Coding Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711dd20e-b662-4d2f-8635-2edaf9ddf351",
   "metadata": {},
   "source": [
    "##### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659eb3d-7e65-4e77-8180-06ace7645314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certainly! Below is a Python function that performs data augmentation on an image dataset using the PIL library. This function includes random rotation, horizontal flipping, and random cropping. The augmentation parameters such as rotation angles, flip probability, and crop size are customizable by the user.\n",
    "\n",
    "# You'll need to have the PIL (Pillow) library installed. If it's not installed, you can install it via pip:\n",
    "\n",
    "pip install Pillow\n",
    "\n",
    "# Here is the function:\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def augment_data(images, rotation_range=20, flip_probability=0.5, crop_size=(100, 100)):\n",
    "    augmented_images = []\n",
    "    for img in images:\n",
    "        # Open the image using PIL\n",
    "        image = Image.open(img)\n",
    "        \n",
    "        # Random Rotation\n",
    "        rotation_angle = random.randint(-rotation_range, rotation_range)\n",
    "        image = image.rotate(rotation_angle)\n",
    "        \n",
    "        # Horizontal flipping\n",
    "        if random.random() < flip_probability:\n",
    "            image = image.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        # Random cropping\n",
    "        width, height = image.size\n",
    "        crop_width, crop_height = crop_size\n",
    "        left = random.randint(0, max(0, width - crop_width))\n",
    "        upper = random.randint(0, max(0, height - crop_height))\n",
    "        right = min(width, left + crop_width)\n",
    "        lower = min(height, upper + crop_height)\n",
    "        image = image.crop((left, upper, right, lower))\n",
    "        \n",
    "        augmented_images.append(image)\n",
    "    \n",
    "    return augmented_images\n",
    "# This function, augment_data, takes a list of image file paths as input and applies random rotation within the specified range, horizontal flipping based on the given probability, and random cropping with the provided crop size.\n",
    "\n",
    "# You can use this function by passing a list of image file paths from your dataset:\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'image_file_paths' with the actual paths of your image dataset\n",
    "image_file_paths = ['image1.jpg', 'image2.jpg', ...]  # Provide actual file paths\n",
    "\n",
    "augmented_dataset = augment_data(image_file_paths, rotation_range=30, flip_probability=0.7, crop_size=(150, 150))\n",
    "# Adjust the rotation_range, flip_probability, and crop_size parameters according to your requirements. This function will return a list of augmented images as PIL Image objects, which you can then use for training your GAN or any other image-related task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c51d5-1840-49bb-a9ae-b835c038812f",
   "metadata": {},
   "source": [
    "#### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d431fe1-c01f-4a2f-89ad-8dd1638b317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example of a simple discriminator model using TensorFlow's Keras that can classify an image as real or fake, based on an image input and an input of random noise of the same size. This kind of discriminator is commonly used in GANs to differentiate between real and generated (fake) images.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    # Input layers for both the real image and the random noise\n",
    "    real_image = Input(shape=input_shape)\n",
    "    generated_image = Input(shape=input_shape)\n",
    "\n",
    "    # Concatenate the real and generated images to process together\n",
    "    combined_images = tf.keras.layers.Concatenate()([real_image, generated_image])\n",
    "\n",
    "    # Convolutional layers for processing the concatenated images\n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(combined_images)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Dense layers for classification\n",
    "    x = Dense(1, activation='sigmoid')(x)  # Output is a single value for binary classification (real/fake)\n",
    "\n",
    "    # Create and compile the model\n",
    "    discriminator = Model(inputs=[real_image, generated_image], outputs=x)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "# Example usage\n",
    "image_shape = (64, 64, 3)  # Replace with the shape of your images\n",
    "discriminator_model = build_discriminator(input_shape=image_shape)\n",
    "discriminator_model.summary()  # See the model summary\n",
    "# This code defines a basic discriminator model in Keras. The build_discriminator function constructs the discriminator network that takes both a real image and a generated image (random noise of the same size as the real image) as inputs and produces a binary classification (real or fake) output. Adjust the input shape to match the shape of the images in your dataset.\n",
    "\n",
    "# You can further train this model with pairs of real and generated images to classify them as real or fake. Adjust the training data and parameters based on your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f64840-819a-4c43-b22d-0d803c7a3ebc",
   "metadata": {},
   "source": [
    "##### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10c77c-bc8d-486f-ba23-c98b6910e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an example architecture for a generator using transpose convolutions to produce 32x32x3 images from random noise (latent space). This network can be used as a part of a Generative Adversarial Network (GAN) to generate images.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Conv2DTranspose\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "\n",
    "    # Project and reshape the input noise\n",
    "    x = Dense(4 * 4 * 512, activation='relu')(noise)\n",
    "    x = Reshape((4, 4, 512))(x)\n",
    "\n",
    "    # Transpose convolutions to upsample the image\n",
    "    x = Conv2DTranspose(256, kernel_size=5, strides=2, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(128, kernel_size=5, strides=2, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(64, kernel_size=5, strides=2, padding='same', activation='relu')(x)\n",
    "    generated_image = Conv2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh')(x)  # Output layer\n",
    "\n",
    "    generator = tf.keras.models.Model(inputs=noise, outputs=generated_image)\n",
    "\n",
    "    return generator\n",
    "\n",
    "# Example usage\n",
    "latent_dim = 100  # Latent space dimension\n",
    "generator_model = build_generator(latent_dim)\n",
    "generator_model.summary()  # See the model summary\n",
    "# This code defines a basic generator model in Keras using transpose convolutions to upsample the input noise to generate 32x32x3 images. Adjust the latent space dimension according to your requirements. This model can be used as a part of a GAN to generate images, with the generator producing images from random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408e8c6-1a4f-4b64-a670-c7f635bd3339",
   "metadata": {},
   "source": [
    "##### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0ee9b-75be-4cba-8783-1aae9024bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Minimax loss function for a GAN involves the binary cross-entropy loss between the generated (fake) and real samples. In this case, the objective is to minimize the discrepancy between the generated samples (where the discriminator predicts \"fake\") and the real samples (where the discriminator predicts \"real\").\n",
    "\n",
    "# Here's a Python function to compute the Minimax loss for a GAN:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def minimax_loss(discriminator_predictions):\n",
    "    # Assuming the discriminator predictions for generated and real samples are given\n",
    "    # discriminator_predictions should be a tuple (predictions_generated, predictions_real)\n",
    "\n",
    "    predictions_generated, predictions_real = discriminator_predictions\n",
    "\n",
    "    # Calculate binary cross-entropy for generated samples\n",
    "    loss_generated = tf.keras.losses.binary_crossentropy(tf.zeros_like(predictions_generated), predictions_generated)\n",
    "\n",
    "    # Calculate binary cross-entropy for real samples\n",
    "    loss_real = tf.keras.losses.binary_crossentropy(tf.ones_like(predictions_real), predictions_real)\n",
    "\n",
    "    # Total Minimax loss as the sum of both losses\n",
    "    minimax_loss = tf.reduce_mean(loss_generated) + tf.reduce_mean(loss_real)\n",
    "\n",
    "    return minimax_loss\n",
    "\n",
    "# Example usage:\n",
    "# Replace with the actual discriminator predictions (scores) for generated and real samples\n",
    "predictions_generated = [...]  # Replace with generated sample scores\n",
    "predictions_real = [...]  # Replace with real sample scores\n",
    "\n",
    "# Call the minimax_loss function with the discriminator predictions\n",
    "loss = minimax_loss((predictions_generated, predictions_real))\n",
    "print(\"Minimax Loss:\", loss.numpy())\n",
    "# This function calculates the Minimax loss by computing the binary cross-entropy losses for both the generated and real samples and then summing up these losses.\n",
    "\n",
    "# Replace [...] in the example usage with the actual predictions (scores) from the discriminator for the generated and real samples. This function assumes that the scores are given as a tuple: (predictions_generated, predictions_real). Adjust the inputs to match the outputs of your discriminator model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bbee9-82af-41ef-9923-c5e3b7ab9262",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e15ea-1eae-47f1-b2ef-362c5f6ec2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create your own GAN model to generate images using the CIFAR-10 dataset, you'll need to implement a generator and a discriminator network and then connect them in a GAN architecture. Here's an example using TensorFlow and Keras:\n",
    "\n",
    "# Steps:\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "\n",
    "# Load the CIFAR-10 dataset and preprocess the images (resize, normalize, etc.).\n",
    "# Create the Generator\n",
    "\n",
    "# Build a generator network that takes random noise as input and produces images. You can use transpose convolutions or upsampling layers to upsample the noise.\n",
    "# Create the Discriminator\n",
    "\n",
    "# Build a discriminator network that takes images as input and predicts whether they are real or fake.\n",
    "# Compile the Discriminator\n",
    "\n",
    "# Compile the discriminator with appropriate loss and optimizer.\n",
    "# Create the GAN\n",
    "\n",
    "# Combine the generator and discriminator to form the GAN.\n",
    "# The generator is trained to fool the discriminator, and the discriminator is trained to distinguish between real and generated images.\n",
    "# Training Loop\n",
    "\n",
    "# Train the GAN in alternating steps: first, the discriminator is trained on both real and generated images, then the generator is trained to produce images that the discriminator classifies as real.\n",
    "# Here's a basic example to get you started:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Reshape, Conv2D, Conv2DTranspose, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "# Replace this section with your code to load and preprocess CIFAR-10 images\n",
    "\n",
    "# Define the Generator\n",
    "def build_generator():\n",
    "    generator = Sequential([\n",
    "        Dense(4 * 4 * 256, input_shape=(100,)),\n",
    "        Reshape((4, 4, 256)),\n",
    "        Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
    "        Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
    "        Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh')\n",
    "    ])\n",
    "    return generator\n",
    "\n",
    "# Define the Discriminator\n",
    "def build_discriminator():\n",
    "    discriminator = Sequential([\n",
    "        Conv2D(64, kernel_size=3, strides=2, input_shape=(32, 32, 3), padding='same', activation='relu'),\n",
    "        Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
    "        Conv2D(256, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return discriminator\n",
    "\n",
    "# Create the models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "# Combine the generator and discriminator to create the GAN\n",
    "discriminator.trainable = False  # This freezes the discriminator during GAN training\n",
    "gan = Sequential([generator, discriminator])\n",
    "gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "# Train the GAN (training loop)\n",
    "# Replace with your training loop using the CIFAR-10 dataset\n",
    "# Train the discriminator, then train the generator to improve its ability to fool the discriminator\n",
    "# This is a basic architecture for a GAN using TensorFlow and Keras. Modify and expand the model architecture, adjust hyperparameters, and run for a sufficient number of epochs to generate meaningful images. Also, ensure proper data loading and augmentation for the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c0c73-acac-48b2-97d6-2e63e0e4bdc3",
   "metadata": {},
   "source": [
    "#### Question6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5250d19-3f52-4ec6-ae8f-d5a462ad5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning with GANs involves using pre-trained convolutional neural network (CNN) models as feature extractors to enhance the performance of Generative Adversarial Networks (GANs) for image generation.\n",
    "\n",
    "# For this task, I'll demonstrate how to use a pre-trained VGG16 model as a feature extractor within the discriminator part of a GAN for generating images from random noise using the CIFAR-10 dataset. This code will use TensorFlow for GAN implementation.\n",
    "\n",
    "# Steps:\n",
    "# Load Pre-trained VGG16 Model\n",
    "# Modify the Model for GAN-Based Image Generation\n",
    "# Implement the Generator and Discriminator Networks\n",
    "# Define the GAN\n",
    "# Training the GAN\n",
    "# Here's a basic example to get you started:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Conv2DTranspose, BatchNormalization, LeakyReLU, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "# Replace this section with your code to load and preprocess CIFAR-10 images\n",
    "\n",
    "# Load pre-trained VGG16 without the top layers\n",
    "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Freezing the layers in VGG16\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the discriminator using VGG16 as feature extractor\n",
    "x = vgg.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256)(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "validity = Dense(1, activation='sigmoid')(x)  # Binary classification - real or fake\n",
    "discriminator = Model(inputs=vgg.input, outputs=validity)\n",
    "\n",
    "# Create the generator\n",
    "generator = tf.keras.Sequential([\n",
    "    Dense(4 * 4 * 256, input_dim=100),\n",
    "    Reshape((4, 4, 256)),\n",
    "    BatchNormalization(),\n",
    "    Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh')\n",
    "])\n",
    "\n",
    "# Create the GAN\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(100,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "# Training loop for the GAN\n",
    "# Replace this section with your training loop using the pre-processed CIFAR-10 dataset\n",
    "# Train the GAN to generate images from random noise using the pre-trained VGG16 discriminator\n",
    "# This code sets up a GAN with the VGG16 model (pre-trained on ImageNet) as a feature extractor in the discriminator. The generator and discriminator are then combined to form the GAN, and the network is trained using the CIFAR-10 dataset.\n",
    "\n",
    "# Please replace the training loop with your own implementation using the CIFAR-10 dataset and consider adjusting hyperparameters for better performance. Additionally, you might need to handle image resizing or normalization as per the requirements of the pre-trained VGG16 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79aab14-7bd2-4af8-83cd-e83eb9b36e1d",
   "metadata": {},
   "source": [
    "#### Question7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308b4c9-3512-4dd3-ac34-5b1b2cdf307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example of a basic Generative Adversarial Network (GAN) implemented in Python using TensorFlow and Keras to generate grayscale images resembling handwritten digits (0 to 9) from the MNIST dataset.\n",
    "\n",
    "#GAN Implementation for MNIST Dataset:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape images for GAN training\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "\n",
    "# Generator model\n",
    "def build_generator(latent_dim):\n",
    "    generator = Sequential([\n",
    "        Dense(128, input_dim=latent_dim),\n",
    "        LeakyReLU(0.2),\n",
    "        Dense(784, activation='tanh'),\n",
    "        Reshape((28, 28, 1))\n",
    "    ])\n",
    "    return generator\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    discriminator = Sequential([\n",
    "        Flatten(input_shape=(28, 28, 1)),\n",
    "        Dense(128),\n",
    "        LeakyReLU(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return discriminator\n",
    "\n",
    "# Create GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan = Sequential()\n",
    "    gan.add(generator)\n",
    "    gan.add(discriminator)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
    "    return gan\n",
    "\n",
    "# Training loop for GAN\n",
    "def train_gan(gan, generator, discriminator, images, latent_dim, epochs=30, batch_size=128):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(images.shape[0] // batch_size):\n",
    "            # Training the discriminator\n",
    "            noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "            generated_images = generator.predict(noise)\n",
    "            real_images = images[np.random.choice(images.shape[0], batch_size, replace=False)]\n",
    "\n",
    "            x_combined = tf.concat([real_images, generated_images], axis=0)\n",
    "            y_combined = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "            discriminator_loss = discriminator.train_on_batch(x_combined, y_combined)\n",
    "\n",
    "            # Training the generator\n",
    "            noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "            y_generated = tf.ones((batch_size, 1))\n",
    "            generator_loss = gan.train_on_batch(noise, y_generated)\n",
    "\n",
    "        print(f'Epoch: {epoch} | Discriminator Loss: {discriminator_loss[0]} | Generator Loss: {generator_loss}')\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# Train the GAN\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "trained_generator = train_gan(gan, generator, discriminator, train_images, latent_dim, epochs=30)\n",
    "\n",
    "# Generate sample images using the trained generator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 5\n",
    "noise = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "generated_images = trained_generator.predict(noise)\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(num_samples):\n",
    "    ax = plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "# This code defines a basic GAN for generating MNIST-like handwritten digits using TensorFlow and Keras. The generator, discriminator, and GAN models are defined and trained on the MNIST dataset. Finally, it generates sample images using the trained generator. Adjust the hyperparameters, models, and training loop as needed for better performance and image quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75044b7-f13e-4d24-9e38-3d708ab7f3b2",
   "metadata": {},
   "source": [
    "#### Question8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e990e-e752-4518-ab01-42f6cb0125d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Deep Convolutional Generative Adversarial Network (DCGAN) in TensorFlow/Keras to generate high-resolution images from low-resolution images involves using convolutional neural networks (CNNs) in both the generator and the discriminator. DCGANs have proven to be effective in generating high-quality images by employing a deep architecture with convolutional layers. Below is an example of a DCGAN architecture for super-resolution tasks:\n",
    "\n",
    "# Architectural Choices and Their Contributions:\n",
    "# Generator Network:\n",
    "\n",
    "# Upsampling Layers: Utilize transposed convolutions (Conv2DTranspose) to upscale the low-resolution images gradually to the desired high resolution. Transposed convolutions help generate high-resolution images from low-resolution inputs.\n",
    "# Batch Normalization: Adding batch normalization after each layer helps in stabilizing and accelerating the training of the network.\n",
    "# ReLU Activation: Using ReLU activation functions in hidden layers helps introduce non-linearity and allows the model to learn complex patterns.\n",
    "# Discriminator Network:\n",
    "\n",
    "# Convolutional Layers: Use multiple convolutional layers to process images and learn features at different levels of abstraction.\n",
    "# Strided Convolutions: Strided convolutions help reduce spatial dimensions, allowing the network to learn more abstract and higher-level features.\n",
    "# LeakyReLU Activation: Implement LeakyReLU activation in the discriminator to introduce non-linearity and prevent the vanishing gradient problem.\n",
    "# Example Implementation:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, BatchNormalization, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the Generator\n",
    "def build_generator():\n",
    "    input_shape = (32, 32, 3)  # Adjust dimensions to match the low-res images\n",
    "    latent_dim = 100  # Adjust as needed\n",
    "\n",
    "    generator = tf.keras.Sequential([\n",
    "        Input(shape=(latent_dim,)),\n",
    "        Dense(8 * 8 * 128),\n",
    "        LeakyReLU(0.2),\n",
    "        Reshape((8, 8, 128)),\n",
    "        Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh')\n",
    "    ])\n",
    "    return generator\n",
    "\n",
    "# Define the Discriminator\n",
    "def build_discriminator():\n",
    "    input_shape = (64, 64, 3)  # Adjust dimensions to match the high-res images\n",
    "\n",
    "    discriminator = tf.keras.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv2D(64, kernel_size=4, strides=2, padding='same'),\n",
    "        LeakyReLU(0.2),\n",
    "        Conv2D(128, kernel_size=4, strides=2, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(0.2),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return discriminator\n",
    "\n",
    "# Combine Generator and Discriminator to form GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan = tf.keras.Sequential()\n",
    "    gan.add(generator)\n",
    "    gan.add(discriminator)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
    "    return gan\n",
    "\n",
    "# Prepare your low-res and high-res image data\n",
    "# Load, preprocess, and prepare the image data from the provided dataset\n",
    "\n",
    "# Train the GAN\n",
    "latent_dim = 100  # Dimension of the latent space\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Training loop - adapt this to your image data\n",
    "# Train the GAN using your low-res and high-res image data\n",
    "\n",
    "# Generate high-res images from low-res inputs using the trained generator\n",
    "# Use the trained generator to upscale low-res images to high resolution\n",
    "# Adjust the network architectures, input/output dimensions, hyperparameters, and the training loop to fit the specifics of the provided low-resolution images dataset for super-resolution tasks. The outlined architectural choices aim to promote feature learning and improve the overall performance of the DCGAN for generating high-resolution images from low-resolution inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf75628-2abf-4a1b-9dd0-8dbb488ef911",
   "metadata": {},
   "source": [
    "##### Question9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cf056-4eb9-415b-af56-996cc6ec1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Conditional Generative Adversarial Network (cGAN) is an extension of the GAN architecture where both the generator and discriminator networks are conditioned on some additional information. In this case, the cGAN will generate images from the Fashion MNIST dataset conditioned on the class label (the type of clothing).\n",
    "\n",
    "# Steps to create a cGAN:\n",
    "# Load the Fashion MNIST dataset from TensorFlow Datasets.\n",
    "# Create the Generator and Discriminator networks conditioned on the class label.\n",
    "# Define the cGAN architecture that combines the generator and discriminator.\n",
    "# Train the cGAN using both real images from the dataset and noise with the corresponding class labels.\n",
    "# Generate images from random noise conditioned on specific class labels using the trained generator.\n",
    "# Here's an example implementation:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize images to the range [-1, 1]\n",
    "\n",
    "# Define the Generator for cGAN\n",
    "def build_generator(latent_dim):\n",
    "    input_label = Input(shape=(1,))\n",
    "    label_embedding = Embedding(10, 50)(input_label)\n",
    "    label_dense = Dense(7 * 7)(label_embedding)\n",
    "    label_reshaped = Reshape((7, 7, 1))(label_dense)\n",
    "\n",
    "    input_noise = Input(shape=(latent_dim,))\n",
    "    noise_dense = Dense(128 * 7 * 7)(input_noise)\n",
    "    noise_reshaped = Reshape((7, 7, 128))(noise_dense)\n",
    "\n",
    "    combined = Concatenate()([noise_reshaped, label_reshaped])\n",
    "    generator = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', activation='relu')(combined)\n",
    "    generator = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', activation='tanh')(generator)\n",
    "    generator = Model(inputs=[input_noise, input_label], outputs=generator)\n",
    "    return generator\n",
    "\n",
    "# Define the Discriminator for cGAN\n",
    "def build_discriminator():\n",
    "    input_image = Input(shape=(28, 28, 1))\n",
    "    input_label = Input(shape=(1,))\n",
    "    label_embedding = Embedding(10, 50)(input_label)\n",
    "    label_dense = Dense(28*28)(label_embedding)\n",
    "    label_reshaped = Reshape((28, 28, 1))(label_dense)\n",
    "\n",
    "    combined = Concatenate()([input_image, label_reshaped])\n",
    "    discriminator = Conv2D(64, (3,3), strides=(2,2), padding='same', activation='relu')(combined)\n",
    "    discriminator = Flatten()(discriminator)\n",
    "    discriminator = Dense(1, activation='sigmoid')(discriminator)\n",
    "    discriminator = Model(inputs=[input_image, input_label], outputs=discriminator)\n",
    "    return discriminator\n",
    "\n",
    "# Combine Generator and Discriminator to form cGAN\n",
    "def build_cgan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    cgan_output = discriminator([generator.output, generator.input[1]])\n",
    "    cgan = Model(inputs=generator.input, outputs=cgan_output)\n",
    "    cgan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
    "    return cgan\n",
    "\n",
    "# Train the cGAN\n",
    "latent_dim = 100  # Dimension of the latent space\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator()\n",
    "cgan = build_cgan(generator, discriminator)\n",
    "\n",
    "# Training loop - adapt this to your data and training requirements\n",
    "# Train the cGAN using real images from the Fashion MNIST dataset and noise with labels\n",
    "\n",
    "# Generate images from noise and specific class labels using the trained generator\n",
    "# Use the trained generator to generate images conditioned on different labels\n",
    "# This code outlines the creation of a conditional Generative Adversarial Network (cGAN) for generating images based on class labels from the Fashion MNIST dataset. The generator and discriminator networks are conditioned on class labels to improve the generation of images based on specific categories. Adjust the architecture, hyperparameters, and training loop to fit the requirements of your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

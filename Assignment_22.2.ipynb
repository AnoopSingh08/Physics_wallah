{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbbe8e46-aa5a-460d-a973-ab1e79af02fd",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4771443-69d4-4527-8288-19b2402039b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection and object classification are two related but distinct tasks in the field of computer vision. They both involve the identification of objects within an image, but they have different goals and challenges.\n",
    "\n",
    "# Object Detection:\n",
    "\n",
    "#     Goal: The primary goal of object detection is to identify and locate objects within an image or a scene and draw bounding boxes around them. In other words, it aims to answer the question, \"Where are the objects in this image, and what are their positions?\"\n",
    "#     Challenges: Object detection involves handling multiple objects of different classes in a single image, dealing with variations in object size, orientation, and occlusion, and distinguishing between objects and background clutter.\n",
    "#     Example: In autonomous driving, object detection is used to identify and locate pedestrians, vehicles, traffic signs, and other objects in the scene, allowing the vehicle to take appropriate actions to avoid collisions.\n",
    "\n",
    "# Object Classification:\n",
    "\n",
    "#     Goal: Object classification focuses on determining the class or category to which an object in an image belongs. It aims to answer the question, \"What is the object in this image, and what category does it belong to?\"\n",
    "#     Challenges: Object classification typically deals with single objects in isolation, assuming that the object of interest is prominent and occupies a significant portion of the image. It must handle variations in object appearance and pose.\n",
    "#     Example: In a medical image analysis application, object classification might involve classifying an X-ray image as either \"normal\" or \"abnormal,\" where \"abnormal\" could encompass various diseases or conditions.\n",
    "\n",
    "# Examples to Illustrate the Difference:\n",
    "\n",
    "#     Object Detection Example:\n",
    "#         In an image of a crowded street, object detection can identify and draw bounding boxes around each pedestrian, bicycle, car, and traffic light present in the scene. It provides not only the class labels (e.g., \"car,\" \"pedestrian\") but also the precise locations of these objects.\n",
    "\n",
    "#     Object Classification Example:\n",
    "#         In a separate scenario, a chest X-ray image is given to an object classifier. The task here is not to locate specific abnormalities within the image but to classify the entire image as either \"normal\" or indicative of a specific condition like pneumonia or tuberculosis.\n",
    "\n",
    "# In summary, object detection involves both identifying objects and locating them within an image, often dealing with multiple objects of different classes. Object classification, on the other hand, is concerned with identifying the class of a single, isolated object without determining its precise location. Both tasks are fundamental in computer vision and find applications in various domains, from autonomous driving to healthcare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da141fc-5a8d-40ae-8110-a28cddb36f30",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9c160-322f-4ac7-9840-128260faef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection techniques are widely used in a variety of real-world applications, offering significant benefits in terms of automation, safety, and efficiency. Here are three scenarios where object detection plays a crucial role:\n",
    "\n",
    "#     Autonomous Driving:\n",
    "#         Significance: In autonomous driving systems, object detection is a critical component for ensuring the safety of passengers, pedestrians, and other road users. It allows self-driving vehicles to perceive and respond to their surroundings.\n",
    "#         Benefits:\n",
    "#             Collision Avoidance: Object detection systems identify and track vehicles, pedestrians, cyclists, and obstacles in real-time, enabling the vehicle to make decisions to avoid collisions.\n",
    "#             Lane Keeping: Detection of lane markings and lane boundaries assists in maintaining proper lane positioning.\n",
    "#             Traffic Sign Recognition: Recognizing traffic signs (e.g., stop signs, speed limits) contributes to safe driving and compliance with traffic rules.\n",
    "#             Enhanced Navigation: Detection of traffic lights and their states helps in obeying traffic signals and planning efficient stops.\n",
    "#             Adaptive Cruise Control: Identifying vehicles ahead and their relative speeds facilitates adaptive cruise control systems to adjust the vehicle's speed accordingly.\n",
    "\n",
    "#     Retail and Inventory Management:\n",
    "#         Significance: In retail and inventory management, object detection is essential for tracking and managing products on store shelves, ensuring inventory accuracy, and improving the shopping experience for customers.\n",
    "#         Benefits:\n",
    "#             Shelf Management: Retailers use object detection to monitor the availability of products on shelves, ensuring that items are restocked in a timely manner.\n",
    "#             Inventory Tracking: Keeping track of inventory levels in real-time helps reduce out-of-stock and overstock situations, optimizing inventory turnover.\n",
    "#             Self-Checkout: Object detection in automated checkout systems allows customers to scan and pay for items without cashier assistance.\n",
    "#             Theft Prevention: Identifying suspicious behavior through object detection helps prevent shoplifting and enhances security.\n",
    "\n",
    "#     Security and Surveillance:\n",
    "#         Significance: Object detection is a key component in security and surveillance systems, providing round-the-clock monitoring and alerting for various applications.\n",
    "#         Benefits:\n",
    "#             Intrusion Detection: Detecting intruders or unauthorized individuals in restricted areas and alerting security personnel or automated systems.\n",
    "#             Perimeter Surveillance: Monitoring the perimeter of properties, industrial sites, and critical infrastructure to detect breaches or suspicious activities.\n",
    "#             Facial Recognition: Identifying and tracking individuals based on facial features for access control or identifying persons of interest.\n",
    "#             Crowd Monitoring: In crowded public spaces, object detection can be used to monitor crowd density, flow, and identify unusual behavior for safety and security purposes.\n",
    "\n",
    "# These scenarios illustrate the significance of object detection in enhancing safety, efficiency, and automation in various domains. The ability to identify and locate objects in real-time, often with the help of deep learning techniques, empowers systems to make informed decisions and take appropriate actions, leading to improved outcomes in these applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbbbff-94c1-4a16-8835-794d11c5393f",
   "metadata": {},
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936dc762-848a-4be9-a31c-e12dda903ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image data can be considered both structured and unstructured, depending on how it is processed and analyzed. Here's an explanation of why image data can be seen as structured and unstructured, along with examples to support each perspective:\n",
    "\n",
    "# Structured Aspect:\n",
    "\n",
    "#     Pixel Grid Structure: At its core, image data is structured in the form of a two-dimensional (2D) or three-dimensional (3D) grid of pixels. Each pixel contains information about color (in the case of RGB images) or grayscale intensity. This structured representation allows for the direct interpretation of images using their pixel values.\n",
    "\n",
    "#     Spatial Relationships: Images have inherent spatial structure. The arrangement of pixels encodes information about the spatial relationships between objects and features within the image. For example, in a medical image, the proximity of tissues and anomalies can be crucial for diagnosis.\n",
    "\n",
    "#     Structured Metadata: Image data often comes with structured metadata, such as image size, resolution, capture date, and camera settings. This additional information enhances the structured aspect of image data.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "#     In a black-and-white digital image of a crossword puzzle, each pixel corresponds to either a filled or empty square, making it a structured representation.\n",
    "#     In satellite imagery, the grid structure of pixels represents geographical features and land cover in a structured manner.\n",
    "\n",
    "# Unstructured Aspect:\n",
    "\n",
    "#     Complex Content: Images can contain complex and diverse content, making them unstructured from a semantic perspective. Recognizing and interpreting objects and scenes within an image often requires advanced pattern recognition and machine learning techniques.\n",
    "\n",
    "#     High-Dimensional Data: Color images, especially high-resolution ones, result in high-dimensional data. The number of pixels in an image can be very large, and directly analyzing all pixel values can be computationally intensive and challenging.\n",
    "\n",
    "#     Subjectivity: The interpretation of image data can be subjective, as different individuals may perceive and label objects or features within an image differently. This subjectivity adds an unstructured dimension to image analysis.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "#     An image of a natural scene contains a wide variety of objects, such as trees, animals, and clouds, making it unstructured in terms of content.\n",
    "#     In facial recognition, the specific arrangement and features of a person's face must be recognized within an image, requiring complex pattern recognition techniques.\n",
    "\n",
    "# In summary, image data can be viewed as a structured form of data due to its pixel grid structure and spatial relationships. However, its content complexity, high dimensionality, and subjectivity contribute to its unstructured nature. The structured or unstructured nature of image data depends on the context and the specific tasks involved, as it can be processed and analyzed in various ways to extract meaningful information and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb8be0-3bce-4751-80eb-e866bcd93b66",
   "metadata": {},
   "source": [
    "### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225e09b-2768-48b5-afb4-141ea2be177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Networks (CNNs) are a class of deep learning models designed specifically for processing and understanding image data. They are highly effective at extracting and understanding information from images through a series of key components and processes:\n",
    "\n",
    "# 1. Convolutional Layers:\n",
    "\n",
    "#     Convolutional layers are the foundation of CNNs. They consist of filters (also known as kernels) that slide over the input image to perform convolution operations.\n",
    "#     Filters are learned weights that capture various features, such as edges, corners, and textures, at different spatial scales.\n",
    "#     Convolutional layers are responsible for feature extraction. By convolving filters with the input image, they create feature maps that highlight relevant image features.\n",
    "\n",
    "# 2. Pooling Layers:\n",
    "\n",
    "#     Pooling layers downsample the spatial dimensions of feature maps while preserving essential information. Common pooling operations include max pooling and average pooling.\n",
    "#     Pooling reduces the computational load and helps the network become more invariant to variations in scale, orientation, and position of features.\n",
    "\n",
    "# 3. Activation Functions:\n",
    "\n",
    "#     Activation functions (e.g., ReLU, Leaky ReLU) introduce non-linearity into the model. They help CNNs model complex relationships between features.\n",
    "#     After convolution and pooling operations, activation functions are applied to the feature maps.\n",
    "\n",
    "# 4. Fully Connected Layers:\n",
    "\n",
    "#     Fully connected layers follow the convolutional and pooling layers. They are traditional neural network layers that learn high-level representations.\n",
    "#     Fully connected layers take flattened feature maps as input and generate predictions based on these representations.\n",
    "\n",
    "# 5. Backpropagation:\n",
    "\n",
    "#     CNNs are trained using backpropagation, a process that adjusts the network's parameters (weights and biases) to minimize a loss function.\n",
    "#     During training, the network compares its predictions to the ground truth labels and calculates the loss.\n",
    "#     Backpropagation propagates the gradient of the loss backward through the network, updating weights to minimize the loss.\n",
    "\n",
    "# 6. Feature Hierarchies:\n",
    "\n",
    "#     CNNs naturally build hierarchical representations of features. Lower layers capture low-level features like edges and textures, while higher layers represent more complex structures and objects.\n",
    "#     The hierarchical approach allows CNNs to progressively abstract and understand the content of an image.\n",
    "\n",
    "# 7. Convolutional Filters:\n",
    "\n",
    "#     CNNs learn to recognize important features by training convolutional filters on a large dataset. These filters capture various aspects of the input image, allowing the network to distinguish between different objects and patterns.\n",
    "\n",
    "# 8. Object Detection and Localization:\n",
    "\n",
    "#     CNNs can perform object detection and localization by identifying regions of interest within an image and drawing bounding boxes around objects.\n",
    "#     This capability is achieved through specialized architectures like Region-based CNNs (R-CNNs) and Single Shot MultiBox Detectors (SSD).\n",
    "\n",
    "# 9. Transfer Learning:\n",
    "\n",
    "#     CNNs can leverage transfer learning, where pre-trained models on large datasets (e.g., ImageNet) are fine-tuned on specific tasks or datasets.\n",
    "#     Transfer learning enables the efficient training of models on smaller datasets while benefiting from the knowledge acquired from the larger dataset.\n",
    "\n",
    "# In summary, CNNs are highly effective at extracting and understanding information from images through a hierarchical process of feature extraction, pooling, non-linearity, and fully connected layers. By learning from data, CNNs become adept at recognizing objects, patterns, and complex structures in images, making them a key technology in computer vision applications such as image classification, object detection, and segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d5221-69eb-4aee-81bd-4a86a276bf78",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955aa64-1a69-4543-adde-c48a612f59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening images and feeding them directly into an Artificial Neural Network (ANN) for image classification is not recommended due to several limitations and challenges:\n",
    "\n",
    "#     Loss of Spatial Information:\n",
    "#         Flattening an image collapses its two-dimensional structure into a one-dimensional vector, resulting in the loss of valuable spatial information. This information is crucial for recognizing patterns and objects within the image.\n",
    "\n",
    "#     Inability to Capture Local Patterns:\n",
    "#         ANNs that accept flattened images cannot effectively capture local patterns, such as edges, textures, and small features. These patterns play a critical role in image classification.\n",
    "\n",
    "#     Dimensionality:\n",
    "#         Flattening images increases the dimensionality of the input data. This high dimensionality can lead to a significantly larger number of parameters in the network, making it computationally expensive and prone to overfitting, especially when working with high-resolution images.\n",
    "\n",
    "#     Computational Cost:\n",
    "#         Flattening large images can lead to a dramatic increase in computational cost, as ANNs need to process and learn from many more parameters. This can make training and inference inefficient and slow.\n",
    "\n",
    "#     Lack of Translation Invariance:\n",
    "#         ANNs for image classification typically lack translation invariance. Flattened images do not capture information about the relative positions of features, making the network sensitive to the location of objects within the image.\n",
    "\n",
    "#     Limited Feature Hierarchies:\n",
    "#         Flattening images prematurely disrupts the development of feature hierarchies that convolutional neural networks (CNNs) naturally build. CNNs use convolutional layers to progressively extract and abstract features, capturing local details and combining them to recognize higher-level patterns and objects.\n",
    "\n",
    "#     Difficulty in Handling Variable-Sized Images:\n",
    "#         Images can come in various sizes, and flattening forces a fixed-size input, requiring resizing or cropping. This process can lead to the distortion of images and information loss.\n",
    "\n",
    "#     Lack of Generalization:\n",
    "#         Flattening does not allow for the generalization of learned features to new, unseen images of different sizes and aspect ratios. Convolutional networks can handle varying image dimensions more effectively.\n",
    "\n",
    "#     Poor Performance:\n",
    "#         Flattened input is likely to result in poor performance in tasks that require understanding complex visual data, such as image classification, object detection, and segmentation.\n",
    "\n",
    "# To overcome these limitations, convolutional neural networks (CNNs) have become the standard approach for image classification tasks. CNNs are designed to preserve spatial information, capture local patterns, and develop feature hierarchies, making them highly effective for understanding and classifying images. They are equipped with convolutional and pooling layers that operate directly on image data, allowing them to learn and recognize features in a spatially aware manner, while significantly reducing the number of parameters compared to flattened input ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173ee8e-c728-4d86-bb6d-f8b737e08c7e",
   "metadata": {},
   "source": [
    "### Question6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dea0a-09ff-4394-8997-2ad31242a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNIST dataset is a collection of handwritten digit images (0-9) commonly used for image classification tasks, particularly in the context of digit recognition. While it is possible to apply Convolutional Neural Networks (CNNs) to the MNIST dataset, there are certain characteristics of the dataset that make it relatively simple and well-suited for classification tasks, reducing the necessity for using CNNs. Here's why CNNs are not necessary for MNIST image classification:\n",
    "\n",
    "#     Low Resolution: MNIST images are relatively low in resolution, with each image typically being 28x28 pixels. This size is small enough to be easily handled by traditional neural networks (Artificial Neural Networks, ANNs) without the need for complex feature extraction mechanisms like CNNs.\n",
    "\n",
    "#     Single-Channel Grayscale Images: MNIST images are grayscale, which means they have only one color channel (as opposed to RGB images with three channels). Grayscale images are simpler to process, as they have fewer color variations, making ANNs effective in extracting relevant features.\n",
    "\n",
    "#     Clear and Consistent Patterns: Handwritten digits in the MNIST dataset are typically well-drawn and centered within the image. There are clear patterns and consistency in the way digits are written, which simplifies the task of feature extraction.\n",
    "\n",
    "#     Low Variability: The MNIST dataset contains digits written by different individuals, but it does not encompass the vast variability present in more complex image datasets. There are limited variations in terms of style, scale, or orientation, reducing the need for sophisticated feature extraction techniques that CNNs provide.\n",
    "\n",
    "#     Small Dataset Size: The MNIST dataset is relatively small by modern deep learning standards, with 60,000 training images and 10,000 test images. Smaller datasets are easier to handle with ANNs, and CNNs are generally employed when dealing with larger and more complex datasets.\n",
    "\n",
    "#     Easily Achievable High Accuracy: Due to its simplicity, MNIST can be accurately classified using basic ANN architectures. Achieving high accuracy on MNIST (above 99%) is common with ANNs, making the use of more complex CNN architectures unnecessary.\n",
    "\n",
    "# In summary, the characteristics of the MNIST dataset, including low resolution, grayscale images, clear patterns, low variability, and a relatively small dataset size, make it amenable to image classification using traditional ANNs. While it is possible to apply CNNs to MNIST, the benefits of using CNNs, such as capturing hierarchical features in complex images, are not fully leveraged in this dataset. Therefore, simpler neural network architectures are often sufficient for achieving excellent results on MNIST. CNNs are better suited for more complex image datasets with higher resolution, variability, and the need for advanced feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5513349a-9b5e-4ecc-a36c-9d3b65d693e5",
   "metadata": {},
   "source": [
    "### Question7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6750810-f965-42df-8ce9-79e4fc2d2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from an image at the local level, rather than considering the entire image as a whole, is a fundamental concept in computer vision and image processing. This approach provides several important advantages and insights:\n",
    "\n",
    "#     Robustness to Variability:\n",
    "#         Local feature extraction enables the detection of patterns, edges, and textures that are often invariant to global transformations such as translation, rotation, and scaling. These local features are more robust to variations in object position, orientation, and size.\n",
    "\n",
    "#     Hierarchical Feature Representation:\n",
    "#         Local feature extraction allows for the construction of hierarchical feature representations. Features at lower levels (e.g., edges, corners) serve as building blocks for higher-level features (e.g., shapes, objects). This hierarchy of features mirrors how humans perceive and understand visual information.\n",
    "\n",
    "#     Spatial Information:\n",
    "#         Local features capture spatial information about the image. The relative arrangement of features can provide insights into the structure and content of the scene, which is critical for tasks like object recognition and scene understanding.\n",
    "\n",
    "#     Localization:\n",
    "#         Local features help locate objects or regions of interest within an image. By identifying salient local regions, the system can determine where objects are located and focus further processing on those regions.\n",
    "\n",
    "#     Efficiency:\n",
    "#         Processing local features is computationally more efficient than considering the entire image. By focusing on relevant regions, the computational load is reduced, which is crucial for real-time applications and resource-constrained devices.\n",
    "\n",
    "#     Contextual Information:\n",
    "#         Local features provide context for recognition and interpretation. For example, identifying the local features of a face, such as eyes, nose, and mouth, allows the system to recognize the face as a whole. Local features convey information about the context and structure of objects.\n",
    "\n",
    "#     Discriminative Power:\n",
    "#         Local features are often more discriminative for classification tasks. They highlight unique characteristics of objects and make it easier to distinguish between different classes. For example, the local texture of a cat's fur or the pattern of stripes on a zebra are distinctive features.\n",
    "\n",
    "#     Scale and Multiscale Analysis:\n",
    "#         Local feature extraction supports multiscale analysis, allowing features to be detected at different scales. This is crucial for recognizing objects of varying sizes within an image.\n",
    "\n",
    "#     Flexibility:\n",
    "#         Local feature extraction can be tailored to the specific requirements of a task. Different types of features can be extracted depending on the characteristics of the data and the goals of the analysis.\n",
    "\n",
    "#     Sensitivity to Anomalies:\n",
    "#         Local feature analysis can identify anomalies or outliers in the image. Deviations from the expected local patterns may indicate the presence of unusual objects or irregularities.\n",
    "\n",
    "# In summary, extracting features from an image at the local level provides a rich source of information that is essential for various computer vision tasks, including object recognition, image segmentation, and scene understanding. This approach leverages the spatial distribution of information, allows for hierarchical feature extraction, and provides robustness to variability, making it a fundamental concept in image analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f8283-6118-4b2b-aab1-97a56039d258",
   "metadata": {},
   "source": [
    "### Question8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a1329-fc5c-4ae4-8453-3cf3c68231c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and max pooling operations are fundamental components of Convolutional Neural Networks (CNNs) that play a crucial role in feature extraction and spatial down-sampling. These operations are essential for enabling CNNs to learn hierarchical features and reduce the spatial dimensions of the input data. Here's how convolution and max pooling contribute to these processes:\n",
    "\n",
    "# Convolution Operation:\n",
    "\n",
    "#     Feature Extraction: Convolution involves sliding a small filter (kernel) over the input image to extract local features. These local features are characterized by the presence of specific patterns, edges, or textures. Convolution helps the network focus on relevant, localized information within the image.\n",
    "\n",
    "#     Spatial Hierarchies: Convolutional layers in a CNN are typically organized in a hierarchy. The initial layers detect low-level features like edges and corners, while deeper layers capture more complex and abstract features. This hierarchical approach mimics how humans perceive and understand visual data, starting with simple features and building up to complex objects.\n",
    "\n",
    "#     Shared Weights: Convolution uses shared weights for different parts of the image. This sharing of weights allows the network to learn the same feature detectors across the entire image, promoting weight sharing and reducing the number of parameters in the network. This weight sharing enables the network to generalize better and be more robust to variations in object location.\n",
    "\n",
    "# Max Pooling Operation:\n",
    "\n",
    "#     Spatial Down-Sampling: Max pooling reduces the spatial dimensions of feature maps. It does this by dividing the feature map into non-overlapping regions and selecting the maximum value within each region. This reduces the size of the feature map, effectively down-sampling it. Spatial down-sampling is essential for managing computational complexity and preventing overfitting.\n",
    "\n",
    "#     Translation Invariance: Max pooling provides translation invariance by preserving the most significant information while discarding less important details. This means that the network can recognize features, patterns, or objects regardless of their precise location within the receptive field, enhancing the network's ability to generalize.\n",
    "\n",
    "#     Noise Reduction: Max pooling helps filter out noise and minor variations in the feature maps. By keeping only the most prominent features, it enhances the network's resistance to irrelevant details and enhances its ability to focus on important structures within the data.\n",
    "\n",
    "#     Increased Receptive Field: As the network progresses through multiple layers of max pooling, the receptive field (the area of the input image that influences a given feature) becomes larger. This allows the network to capture more global information and higher-level features.\n",
    "\n",
    "# In summary, convolution and max pooling operations in CNNs work in tandem to extract local features and reduce spatial dimensions progressively. The hierarchical feature extraction and spatial down-sampling achieved through these operations help CNNs recognize patterns and objects efficiently, making them particularly well-suited for image-related tasks like image classification, object detection, and image segmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

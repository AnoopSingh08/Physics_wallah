{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f033e4-4bf5-4957-8c17-107cf740e00e",
   "metadata": {},
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87f7fc-a5a0-4119-ba77-14e34e1021eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the steps to install and load the latest versions of TensorFlow and Keras using Python:\n",
    "\n",
    "#     Install TensorFlow:\n",
    "\n",
    "#     You can use pip, a Python package manager, to install TensorFlow. Open a terminal or command prompt and run the following command:\n",
    "\n",
    "# pip install tensorflow\n",
    "\n",
    "# This command will download and install the latest version of TensorFlow available on the Python Package Index (PyPI).\n",
    "\n",
    "# Install Keras (as part of TensorFlow):\n",
    "\n",
    "# Starting from TensorFlow 2.0, Keras is included as part of TensorFlow, so you don't need to install Keras separately. When you install TensorFlow using the above command, it includes Keras.\n",
    "\n",
    "# Load TensorFlow and Keras in Python:\n",
    "\n",
    "# You can load TensorFlow and Keras in a Python script or Jupyter Notebook. Here's an example of how to do it:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print the versions of TensorFlow and Keras\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "\n",
    "#Running this code will print the versions of TensorFlow and Keras installed on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a1ac3-8dea-40de-aada-21c6a6f006de",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97603cb-0536-4a2c-8d54-ad7de805cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Dataset:\n",
    "\n",
    "#     Download the Wine Quality dataset from Kaggle using the provided link.\n",
    "#     Save the dataset file (e.g., \"winequality.csv\") to your local directory.\n",
    "\n",
    "# Install Required Libraries:\n",
    "\n",
    "#     Ensure you have the required Python libraries installed. You can install them using pip if you haven't already:\n",
    "\n",
    "# pip install pandas numpy\n",
    "\n",
    "# Load and Explore the Dataset:\n",
    "\n",
    "#     Use the following Python code to load and explore the dataset:\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset_path = \"winequality.csv\"  # Replace with the actual path to your dataset file\n",
    "    wine_data = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Explore the dimensions of the dataset\n",
    "    rows, columns = wine_data.shape\n",
    "    print(\"Number of rows:\", rows)\n",
    "    print(\"Number of columns:\", columns)\n",
    "\n",
    "    # Display the first few rows of the dataset\n",
    "    print(wine_data.head())\n",
    "\n",
    "#    Replace \"winequality.csv\" with the actual path to the downloaded dataset file. This code will load the dataset into a pandas DataFrame and print the number of rows, columns, and the first few rows of data.\n",
    "\n",
    "# By following these steps, you can load and explore the Wine Quality dataset in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c929b-774b-4824-876c-8bf8ca0bf0d4",
   "metadata": {},
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb4f27-6e91-4b73-96d1-674ad48e39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for null values, identify categorical variables, and encode them in a dataset using Python, you can use the pandas library for data manipulation and preprocessing. Here's a step-by-step guide:\n",
    "\n",
    "#     Check for Null Values:\n",
    "\n",
    "#     You can use the isnull() method to identify null values in the dataset. Here's how to check for null values and count them:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"winequality.csv\"  # Replace with the actual path to your dataset file\n",
    "wine_data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Check for null values\n",
    "null_counts = wine_data.isnull().sum()\n",
    "print(\"Null Value Counts:\")\n",
    "print(null_counts)\n",
    "\n",
    "# This code will display the count of null values for each column in the dataset.\n",
    "\n",
    "# Identify Categorical Variables:\n",
    "\n",
    "# To identify categorical variables, you can check the data types of each column. Columns with data types such as object or category are often categorical. Here's how to identify categorical columns:\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = wine_data.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"Categorical Columns:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "# This code will print the names of columns that have categorical data.\n",
    "\n",
    "# Encode Categorical Variables:\n",
    "\n",
    "# You can use one-hot encoding to convert categorical variables into numerical format. Pandas provides a convenient function pd.get_dummies() to perform one-hot encoding. Here's how to encode categorical variables:\n",
    "\n",
    "# Encode categorical variables\n",
    "wine_data_encoded = pd.get_dummies(wine_data, columns=categorical_columns)\n",
    "\n",
    "# This code will create a new DataFrame wine_data_encoded with one-hot encoded categorical variables.\n",
    "\n",
    "# Save the Encoded Dataset (Optional):\n",
    "\n",
    "# If you want to save the encoded dataset to a new CSV file, you can use the following code:\n",
    "\n",
    "    # Save the encoded dataset to a new CSV file\n",
    "    encoded_dataset_path = \"winequality_encoded.csv\"  # Replace with your desired file path\n",
    "    wine_data_encoded.to_csv(encoded_dataset_path, index=False)\n",
    "\n",
    "#    This code will save the encoded dataset to a new CSV file named \"winequality_encoded.csv\" (you can choose a different name).\n",
    "\n",
    "# By following these steps, you can check for null values, identify categorical variables, and encode them in the Wine Quality dataset or any other dataset using Python and pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf6da6-7f2e-4625-b5d5-ebf543b73c4e",
   "metadata": {},
   "source": [
    "### Question4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9d82d-e8cf-4bca-a572-d953abe1e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate the features and target variables from a pandas DataFrame, you can use the indexing capabilities of pandas. In your case, you can assume that all columns except the target variable are features. Here's how to do it:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"winequality_encoded.csv\"  # Replace with the path to your dataset file\n",
    "wine_data_encoded = pd.read_csv(dataset_path)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = wine_data_encoded.drop(columns=['target_column_name'])  # Replace 'target_column_name' with the actual name of the target column\n",
    "y = wine_data_encoded['target_column_name']  # Replace 'target_column_name' with the actual name of the target column\n",
    "\n",
    "# Verify the separation\n",
    "print(\"Features (X):\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\nTarget variable (y):\")\n",
    "print(y.head())\n",
    "\n",
    "# Replace \"target_column_name\" with the actual name of the target column in your dataset. This code will create two separate DataFrames, X for features and y for the target variable, allowing you to use them for machine learning tasks like model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fecc9c-f562-496b-89e7-e46f42e4cc4f",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386111e7-a3ab-41a6-9237-5467d1c89daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform a train-test split and divide your data into training, validation, and test datasets, you can use the train_test_split function from the scikit-learn library. Here's how to do it:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Training data shape (X_train):\", X_train.shape)\n",
    "print(\"Training labels shape (y_train):\", y_train.shape)\n",
    "print(\"\\nValidation data shape (X_val):\", X_val.shape)\n",
    "print(\"Validation labels shape (y_val):\", y_val.shape)\n",
    "print(\"\\nTest data shape (X_test):\", X_test.shape)\n",
    "print(\"Test labels shape (y_test):\", y_test.shape)\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We first split the data into a training set (X_train, y_train) and a test set (X_test, y_test) using train_test_split. We allocate 20% of the data for testing (test_size=0.2) and use a random seed (random_state=42) for reproducibility.\n",
    "\n",
    "#     Next, we further split the training set into a training set (X_train, y_train) and a validation set (X_val, y_val) using another train_test_split. We allocate 10% of the original data for validation (test_size=0.1) within the training set.\n",
    "\n",
    "#     Finally, we print the sizes of the resulting datasets to verify the split.\n",
    "\n",
    "# Now, you have your data divided into training, validation, and test sets, which you can use for model training, hyperparameter tuning, and evaluation. Adjust the test_size parameter to control the size of your validation and test sets according to your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c8e768-c195-4daf-9ce5-8091c4a61532",
   "metadata": {},
   "source": [
    "### Question6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cf8b4-021f-4fd1-ba7d-72964aa33e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling is an essential preprocessing step for many machine learning algorithms to ensure that features are on a similar scale. You can use various scaling methods, such as Min-Max scaling (also known as normalization) or Standardization (Z-score scaling). Below, I'll show you how to perform both types of scaling on your dataset using Python and scikit-learn:\n",
    "\n",
    "# Min-Max Scaling (Normalization):\n",
    "\n",
    "# Min-Max scaling scales the features to a specified range, usually [0, 1]. Here's how to perform Min-Max scaling using scikit-learn:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test data using the same scaler\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#After scaling, X_train_scaled, X_val_scaled, and X_test_scaled will contain the scaled feature values.\n",
    "\n",
    "# Standardization (Z-score Scaling):\n",
    "\n",
    "# Standardization scales the features to have a mean of 0 and a standard deviation of 1. Here's how to perform standardization using scikit-learn:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test data using the same scaler\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Similarly, after scaling, X_train_scaled, X_val_scaled, and X_test_scaled will contain the standardized feature values.\n",
    "\n",
    "# Choose the scaling method that best fits your specific machine learning algorithm and problem. Min-Max scaling (Normalization) is often used when you want to constrain the features to a specific range, while Standardization is suitable when you want features to have a mean of 0 and a standard deviation of 1, which is helpful for algorithms that assume Gaussian-distributed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf63cc-80ba-452d-b21e-c4c37039b637",
   "metadata": {},
   "source": [
    "### Question7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9991c9-16c0-41c6-9158-4c8703ac1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a neural network with at least two hidden layers and an output layer for binary categorical variables, you can use popular deep learning libraries like TensorFlow and Keras. Here's an example of how to build such a network:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    # Input layer (specify input_shape for the first layer)\n",
    "    keras.layers.Input(shape=(input_dim,)),  # Replace 'input_dim' with the number of input features\n",
    "    \n",
    "    # First hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # 64 units, ReLU activation\n",
    "    \n",
    "    # Second hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # 32 units, ReLU activation\n",
    "    \n",
    "    # Output layer (for binary classification, use 'sigmoid' activation)\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 1 unit, Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # You can choose a different optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We use the Sequential API in Keras to define a feedforward neural network.\n",
    "\n",
    "#     The Input layer specifies the input shape, which should match the number of input features in your dataset.\n",
    "\n",
    "#     We create two hidden layers with 64 and 32 units, respectively, using the Dense layers. You can adjust the number of units as needed.\n",
    "\n",
    "#     For binary classification, we use an output layer with a single unit and a sigmoid activation function.\n",
    "\n",
    "#     The model is compiled with the Adam optimizer, binary cross-entropy loss (suitable for binary classification), and accuracy as a metric.\n",
    "\n",
    "#     You should replace 'input_dim' with the actual number of input features in your dataset.\n",
    "\n",
    "# You can then proceed to train this model using your training data and evaluate its performance. Additionally, you can customize the model architecture, add more hidden layers, and adjust hyperparameters to better suit your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd40ff0-56e3-411e-96b3-48b088909596",
   "metadata": {},
   "source": [
    "### Question8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfaa25c-116e-4bdd-a3e3-5e37772febe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a Sequential model and add all the layers to it, you can follow the example provided earlier. Here's the code to create a Sequential model and add input, hidden, and output layers:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    # Input layer (specify input_shape for the first layer)\n",
    "    keras.layers.Input(shape=(input_dim,)),  # Replace 'input_dim' with the number of input features\n",
    "    \n",
    "    # First hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # 64 units, ReLU activation\n",
    "    \n",
    "    # Second hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # 32 units, ReLU activation\n",
    "    \n",
    "    # Output layer (for binary classification, use 'sigmoid' activation)\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 1 unit, Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # You can choose a different optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# This code defines a Sequential model and adds an input layer, two hidden layers, and an output layer. You can customize the number of units in the hidden layers, the activation functions, and other hyperparameters to suit your specific problem.\n",
    "\n",
    "# Replace 'input_dim' with the actual number of input features in your dataset. Once the model is created, you can proceed to train and evaluate it using your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c144b4d-6d9c-498a-b547-4629ddb3194d",
   "metadata": {},
   "source": [
    "### Question9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd0bd7-26ad-4595-9fef-608b2422424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement a TensorBoard callback to visualize and monitor the model's training process in TensorFlow and Keras, you can use the TensorBoard callback provided by TensorFlow. Here's how to do it:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    # Input layer (specify input_shape for the first layer)\n",
    "    keras.layers.Input(shape=(input_dim,)),  # Replace 'input_dim' with the number of input features\n",
    "    \n",
    "    # First hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # 64 units, ReLU activation\n",
    "    \n",
    "    # Second hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # 32 units, ReLU activation\n",
    "    \n",
    "    # Output layer (for binary classification, use 'sigmoid' activation)\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 1 unit, Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # You can choose a different optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n",
    "# Train the model with the TensorBoard callback\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    callbacks=[tensorboard_callback])\n",
    "\n",
    "# Optionally, save the model\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We define the model as shown in the previous examples.\n",
    "\n",
    "#     We create a TensorBoard callback and specify the log directory where the logs will be stored (in this case, \"logs\").\n",
    "\n",
    "#     During model training, we pass the tensorboard_callback to the callbacks parameter in the fit method.\n",
    "\n",
    "#     TensorBoard logs will be generated during training in the specified directory.\n",
    "\n",
    "# To launch TensorBoard and visualize the training process, you can use the following command in your terminal or command prompt:\n",
    "\n",
    "# tensorboard --logdir=./logs\n",
    "\n",
    "# This command will start TensorBoard, and you can access it in your web browser by navigating to the URL shown in the terminal.\n",
    "\n",
    "# By using the TensorBoard callback, you can monitor various aspects of the training process, including loss, accuracy, and more, and visualize them in real-time during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9859d3b-9cb3-4d92-b1da-21e98dd51cf6",
   "metadata": {},
   "source": [
    "### Question10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7efc18-5d04-4b32-ac63-3cfe12e73606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping is a technique used to prevent overfitting by monitoring a chosen metric and stopping training if no improvement is observed on a validation set. Here's how to implement early stopping using TensorFlow and Keras:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    # Input layer (specify input_shape for the first layer)\n",
    "    keras.layers.Input(shape=(input_dim,)),  # Replace 'input_dim' with the number of input features\n",
    "    \n",
    "    # First hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # 64 units, ReLU activation\n",
    "    \n",
    "    # Second hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # 32 units, ReLU activation\n",
    "    \n",
    "    # Output layer (for binary classification, use 'sigmoid' activation)\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 1 unit, Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # You can choose a different optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',  # Monitor validation loss\n",
    "                                       patience=5,          # Number of epochs with no improvement before stopping\n",
    "                                       restore_best_weights=True)  # Restore best weights when training stops\n",
    "\n",
    "# Train the model with the EarlyStopping callback\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    callbacks=[early_stopping_callback])\n",
    "\n",
    "# Optionally, save the model\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We define the model as shown in previous examples.\n",
    "\n",
    "#     We create an EarlyStopping callback and specify the metric to monitor (val_loss in this case, which is the validation loss), the patience (number of epochs with no improvement before stopping), and restore_best_weights to restore the model's weights to the best epoch when training stops.\n",
    "\n",
    "#     During model training, we pass the early_stopping_callback to the callbacks parameter in the fit method.\n",
    "\n",
    "#     Early stopping will monitor the validation loss and stop training when no improvement is observed for the specified number of epochs.\n",
    "\n",
    "# By using early stopping, you can prevent overfitting and save training time by automatically stopping training when the model's performance on the validation set plateaus or worsens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e9195-c524-4ea3-aa87-13f673b86b55",
   "metadata": {},
   "source": [
    "### Question11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d4b68-8512-489c-b61a-c4cf29d4e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ModelCheckpoint callback in TensorFlow and Keras allows you to save the best model based on a chosen metric during training. Here's how to implement the ModelCheckpoint callback:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    # Input layer (specify input_shape for the first layer)\n",
    "    keras.layers.Input(shape=(input_dim,)),  # Replace 'input_dim' with the number of input features\n",
    "    \n",
    "    # First hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # 64 units, ReLU activation\n",
    "    \n",
    "    # Second hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # 32 units, ReLU activation\n",
    "    \n",
    "    # Output layer (for binary classification, use 'sigmoid' activation)\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 1 unit, Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # You can choose a different optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath='best_model.h5',  # Filepath to save the best model\n",
    "                                            monitor='val_loss',  # Monitor validation loss\n",
    "                                            save_best_only=True,  # Save only the best model\n",
    "                                            mode='min',  # Minimize the monitored metric (e.g., validation loss)\n",
    "                                            verbose=1)  # Verbosity level (1: display messages)\n",
    "\n",
    "# Train the model with the ModelCheckpoint callback\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# Optionally, load the best model saved during training\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We define the model as shown in previous examples.\n",
    "\n",
    "#     We create a ModelCheckpoint callback and specify the following parameters:\n",
    "#         filepath: The filepath to save the best model.\n",
    "#         monitor: The metric to monitor (e.g., 'val_loss' for validation loss).\n",
    "#         save_best_only: If set to True, only the best model will be saved.\n",
    "#         mode: The mode for comparing the monitored metric. Use 'min' for loss metrics and 'max' for accuracy or other metrics.\n",
    "#         verbose: The verbosity level (1 to display messages during saving).\n",
    "\n",
    "#     During model training, we pass the model_checkpoint_callback to the callbacks parameter in the fit method.\n",
    "\n",
    "#     The ModelCheckpoint callback will save the best model based on the monitored metric.\n",
    "\n",
    "#     Optionally, you can load the best model saved during training using keras.models.load_model('best_model.h5').\n",
    "\n",
    "# By using the ModelCheckpoint callback, you can automatically save the best model and avoid overwriting it during training based on your chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e260e7-0bd5-48c8-b19f-71c15b48375f",
   "metadata": {},
   "source": [
    "### Question12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c785c1-a681-4ac9-83f1-5547034aa528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can print the model summary using the summary() method of your Keras model. Here's how you can do it:\n",
    "\n",
    "\n",
    "# Assuming you have already defined and compiled your model\n",
    "model.summary()\n",
    "\n",
    "# Place this code after you have defined and compiled your model, and it will print the model summary, including the layer names, output shapes, and the number of trainable parameters. This summary provides a useful overview of your model's architecture and can help you verify its structure and the number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b698d-c18a-4616-8676-32c852c7d967",
   "metadata": {},
   "source": [
    "### Question13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd61b3-dfaf-41f7-975c-74fbdd0a1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You've already correctly configured the loss function as binary cross-entropy and included the metric 'accuracy' when you compiled your model. Here's the relevant part of your code:\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # Adam optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])  # Accuracy as a metric\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     The optimizer is set to 'adam,' which is the Adam optimizer.\n",
    "#     The loss function is specified as 'binary_crossentropy,' which is suitable for binary classification tasks.\n",
    "#     The 'accuracy' metric is added to evaluate the model's accuracy during training and evaluation.\n",
    "\n",
    "# So, your model is already configured with the specified loss function, optimizer, and metric. You can proceed with training the model using these settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2b903-6969-473b-987a-3ef3fb40a5d5",
   "metadata": {},
   "source": [
    "### Question14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95e45d-a80e-4eca-a820-8c73cec56533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compile the model with the specified loss function, optimizer, and metrics, you can use the following code:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    # Input layer (specify input_shape for the first layer)\n",
    "    keras.layers.Input(shape=(input_dim,)),  # Replace 'input_dim' with the number of input features\n",
    "    \n",
    "    # First hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # 64 units, ReLU activation\n",
    "    \n",
    "    # Second hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # 32 units, ReLU activation\n",
    "    \n",
    "    # Output layer (for binary classification, use 'sigmoid' activation)\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 1 unit, Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with specified loss, optimizer, and metrics\n",
    "model.compile(optimizer='adam',  # Adam optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])  # Accuracy as a metric\n",
    "\n",
    "# In this code, I've added the model.compile section explicitly to ensure that the model is compiled with the specified loss function ('binary_crossentropy'), optimizer ('adam'), and metric ('accuracy')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1c8ae-a663-4099-a71e-363140b5bd80",
   "metadata": {},
   "source": [
    "### Question15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00417a9c-2a96-48ff-bb28-043039d9d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fit the model to the data while incorporating the TensorBoard, Early Stopping, and ModelCheckpoint callbacks, you can use the following code:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    # Input layer (specify input_shape for the first layer)\n",
    "    keras.layers.Input(shape=(input_dim,)),  # Replace 'input_dim' with the number of input features\n",
    "    \n",
    "    # First hidden layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # 64 units, ReLU activation\n",
    "    \n",
    "    # Second hidden layer\n",
    "    keras.layers.Dense(32, activation='relu'),  # 32 units, ReLU activation\n",
    "    \n",
    "    # Output layer (for binary classification, use 'sigmoid' activation)\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 1 unit, Sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',  # Adam optimizer\n",
    "              loss='binary_crossentropy',  # Binary cross-entropy loss for binary classification\n",
    "              metrics=['accuracy'])  # Accuracy as a metric\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',  # Monitor validation loss\n",
    "                                       patience=5,          # Number of epochs with no improvement before stopping\n",
    "                                       restore_best_weights=True)  # Restore best weights when training stops\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath='best_model.h5',  # Filepath to save the best model\n",
    "                                            monitor='val_loss',  # Monitor validation loss\n",
    "                                            save_best_only=True,  # Save only the best model\n",
    "                                            mode='min',  # Minimize the monitored metric (e.g., validation loss)\n",
    "                                            verbose=1)  # Verbosity level (1: display messages)\n",
    "\n",
    "# Train the model with all the callbacks\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback])\n",
    "\n",
    "# Optionally, load the best model saved during training\n",
    "best_model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We define the model as shown in previous examples.\n",
    "\n",
    "#     We compile the model with the specified loss function, optimizer, and metrics.\n",
    "\n",
    "#     We create the TensorBoard, EarlyStopping, and ModelCheckpoint callbacks as previously explained.\n",
    "\n",
    "#     During model training, we pass all three callbacks to the callbacks parameter in the fit method.\n",
    "\n",
    "#     The model will be trained, and the callbacks will take care of logging TensorBoard data, early stopping if needed, and saving the best model.\n",
    "\n",
    "#     Optionally, you can load the best model saved during training using keras.models.load_model('best_model.h5').\n",
    "\n",
    "# This code ensures that you train your model while leveraging these three important callbacks for monitoring, early stopping, and model checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba962d-255d-4810-9c00-9fc90da2dc0c",
   "metadata": {},
   "source": [
    "### Question16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0054b-c775-4d77-a2b0-cdcb9820f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can retrieve the model's parameters, also known as its weights and biases, using the get_weights() method of the Keras model. This method returns a list of Numpy arrays containing the weights and biases for each layer of the model. Here's how you can get the model's parameters:\n",
    "\n",
    "# Get the model's parameters (weights and biases)\n",
    "model_parameters = model.get_weights()\n",
    "\n",
    "# Display the parameters for each layer\n",
    "for layer_num, layer_params in enumerate(model_parameters):\n",
    "    print(f\"Layer {layer_num + 1} Parameters:\")\n",
    "    print(layer_params)\n",
    "\n",
    "# The model_parameters variable will contain a list of Numpy arrays, and each array corresponds to the parameters of a layer. You can loop through the list to inspect the parameters for each layer.\n",
    "\n",
    "# Please note that the output will be in the form of Numpy arrays, and the interpretation of these arrays depends on the layer type and shape. Weight matrices and bias vectors will be included in these arrays for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c103bc6-417a-4034-9566-8b67319b1cd7",
   "metadata": {},
   "source": [
    "### Question17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbccd9-89cb-4518-8e57-2417c5b4b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can store the model's training history as a Pandas DataFrame by converting the history object returned by the fit method into a DataFrame. Here's how you can do it:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already trained the model and have a 'history' object\n",
    "\n",
    "# Convert the 'history' dictionary into a Pandas DataFrame\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(history_df.head())\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "history_df.to_csv('training_history.csv', index=False)\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We assume that you have already trained the model and obtained a history object from the fit method.\n",
    "\n",
    "#     We convert the history dictionary, which contains training metrics over epochs, into a Pandas DataFrame using pd.DataFrame(history.history).\n",
    "\n",
    "#     You can display the first few rows of the DataFrame to inspect the training history.\n",
    "\n",
    "#     Optionally, you can save the DataFrame to a CSV file using to_csv for further analysis or visualization.\n",
    "\n",
    "# This will allow you to access and analyze various training metrics, such as training loss, validation loss, training accuracy, and validation accuracy, in a structured Pandas DataFrame format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e233a-3605-455e-a75a-11af4753c8f2",
   "metadata": {},
   "source": [
    "#### Question18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b2e02-08e5-4dec-8305-1b3fb2b23174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the model's training history, including training and validation loss, as well as training and validation accuracy, you can use popular data visualization libraries like Matplotlib. Here's how you can do it:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already trained the model and have a 'history' object\n",
    "\n",
    "# Create subplots for loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We assume that you have already trained the model and have a history object from the fit method.\n",
    "\n",
    "#     We create subplots for loss and accuracy using plt.subplot.\n",
    "\n",
    "#     We use plt.plot to plot the training and validation loss in the left subplot and the training and validation accuracy in the right subplot.\n",
    "\n",
    "#     We set titles, labels, and legends to make the plots informative.\n",
    "\n",
    "#     Finally, we use plt.tight_layout() to ensure proper spacing between subplots and plt.show() to display the plots.\n",
    "\n",
    "# This code will generate two subplots showing the training and validation loss as well as the training and validation accuracy over epochs, helping you visualize the model's training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637e003-3c21-41c2-a828-8a80ec3e5f7e",
   "metadata": {},
   "source": [
    "### Question19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112f721-8f74-4f33-b019-2fd8eab510de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate the model's performance using the test data, you can use the evaluate method of your trained Keras model. Here's how you can do it:\n",
    "\n",
    "# Assuming you have already trained the model and loaded the test data (X_test_scaled, y_test)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# In this code:\n",
    "\n",
    "#     We assume that you have already trained the model and loaded the test data (scaled as needed, e.g., X_test_scaled).\n",
    "\n",
    "#     We use the evaluate method to calculate the test loss and accuracy of the model on the test data.\n",
    "\n",
    "#     We print the test loss and accuracy to assess the model's performance on unseen data.\n",
    "\n",
    "# This will provide you with information about how well the model generalizes to new, unseen examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
